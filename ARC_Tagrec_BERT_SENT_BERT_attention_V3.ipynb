{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ARC_Tagrec_BERT_SENT_BERT_attention_V3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "346b92be09eb417981366479c69882ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42b4c9138372408faea573bc15b647da",
              "IPY_MODEL_871916844fa74f7b8e630392f2de16d2",
              "IPY_MODEL_dcc905a354b442c7b0328d714f151c5f"
            ],
            "layout": "IPY_MODEL_f104cccf33f443ac92aab685259ed124"
          }
        },
        "42b4c9138372408faea573bc15b647da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f644b0e48b34460292a002037faf5975",
            "placeholder": "​",
            "style": "IPY_MODEL_3746db1fa8104e09b5b753c04cf47e5c",
            "value": "Downloading: 100%"
          }
        },
        "871916844fa74f7b8e630392f2de16d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c63fd59f2354fa5b06e9fbbcdd8c40c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_972e28e267cb44d7a7be5c90e8b98a89",
            "value": 231508
          }
        },
        "dcc905a354b442c7b0328d714f151c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1eca407adcd43c19e08af1040b6cadd",
            "placeholder": "​",
            "style": "IPY_MODEL_e88f57a808444be5a9b56c6f2fe4ccac",
            "value": " 232k/232k [00:00&lt;00:00, 1.51MB/s]"
          }
        },
        "f104cccf33f443ac92aab685259ed124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f644b0e48b34460292a002037faf5975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3746db1fa8104e09b5b753c04cf47e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c63fd59f2354fa5b06e9fbbcdd8c40c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "972e28e267cb44d7a7be5c90e8b98a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1eca407adcd43c19e08af1040b6cadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88f57a808444be5a9b56c6f2fe4ccac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR9av2JU3kf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b484f1-ed14-4cff-dcae-7e19bc6ff89c"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beu-zA1vy43M"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da-zaZJUGMFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bfa33b-82f0-4ba0-a539-71c0ef0ae09f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbjjEyCh1lf2",
        "outputId": "6b7ecac3-0373-40f8-ca82-fa398e0f7462"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "! pip install tensorflow-hub==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 77 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.46.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.2.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-hub==0.7.0\n",
            "  Downloading tensorflow_hub-0.7.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.15.0)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.12.0\n",
            "    Uninstalling tensorflow-hub-0.12.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.12.0\n",
            "Successfully installed tensorflow-hub-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZnjEXG5_-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573e06e3-b014-4839-ee90-5c953f8020f7"
      },
      "source": [
        "!pip install sentence-transformers==0.2.6.1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers==0.2.6.1\n",
            "  Downloading sentence-transformers-0.2.6.1.tar.gz (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.2.6.1) (3.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->sentence-transformers==0.2.6.1) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.2.6.1) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.8.0->sentence-transformers==0.2.6.1) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.2.6.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.2.6.1) (3.1.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-py3-none-any.whl size=74031 sha256=2c1e9ff95e073c016e5ee81fdf642bcbf9945ec467fda8b3f66e12d879d77431\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/eb/84/05830bceaeef549ceb0257c6797254173e197e971b3f911ee4\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 sentence-transformers-0.2.6.1 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlE6vskY9VmG"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_SENT_BERT_cos_QC_attention_V3/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Xz7HNHZYeo"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_SENT_BERT_cos_attention_2_V3/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKeqoCs3kgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b39ff6-51a6-4880-ef3b-f9d72504f054"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.8.0\n",
            "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
            "\u001b[K     |████████████████████████████████| 563 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.24.2-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 66.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.64.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.2\n",
            "  Downloading botocore-1.27.2-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 54.5 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.2->boto3->transformers==2.8.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.2->boto3->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 79.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ddb319c8b2402d4bb2f65c8b362f5a93c884ecc979e1dda868d761e0ae69c672\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.19.2\n",
            "    Uninstalling transformers-4.19.2:\n",
            "      Successfully uninstalled transformers-4.19.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.24.2 botocore-1.27.2 jmespath-1.0.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.8.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf-OXnTs-ZhS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZFv4UU8sLTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1c47ff-5711-4751-d5c4-06e551676e05"
      },
      "source": [
        "!pip install git+https://github.com/geoopt/geoopt.git\n",
        "! pip install git+https://github.com/ferrine/hyrnn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/geoopt/geoopt.git\n",
            "  Cloning https://github.com/geoopt/geoopt.git to /tmp/pip-req-build-s11_etx0\n",
            "  Running command git clone -q https://github.com/geoopt/geoopt.git /tmp/pip-req-build-s11_etx0\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from geoopt==0.4.1) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from geoopt==0.4.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.9.0->geoopt==0.4.1) (4.2.0)\n",
            "Building wheels for collected packages: geoopt\n",
            "  Building wheel for geoopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for geoopt: filename=geoopt-0.4.1-py3-none-any.whl size=86950 sha256=567c7522f2af17287750ec5039f98a77fb4ed7acf769bd5859b483526b746d17\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bk213zy6/wheels/e9/b9/1f/4238e702b8889c5329cc24bb3c1d020ab33de17df8ee1e8da4\n",
            "Successfully built geoopt\n",
            "Installing collected packages: geoopt\n",
            "Successfully installed geoopt-0.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ferrine/hyrnn.git\n",
            "  Cloning https://github.com/ferrine/hyrnn.git to /tmp/pip-req-build-0ycf3mf1\n",
            "  Running command git clone -q https://github.com/ferrine/hyrnn.git /tmp/pip-req-build-0ycf3mf1\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from hyrnn==0.0.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyrnn==0.0.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->hyrnn==0.0.0) (4.2.0)\n",
            "Building wheels for collected packages: hyrnn\n",
            "  Building wheel for hyrnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyrnn: filename=hyrnn-0.0.0-py3-none-any.whl size=13968 sha256=5c922a7c515e80351c5e3ca43bf5ebc796788db2baa2c5b30ba723a15f6cb709\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_hkes97f/wheels/9a/b5/0c/65c228cf8105d16c7ce153a243913604c5f2312980a719e1b2\n",
            "Successfully built hyrnn\n",
            "Installing collected packages: hyrnn\n",
            "Successfully installed hyrnn-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsADhaO93kgD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd4b0c1d-d7b2-49df-d5fc-00d6c86decf2"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_QC_data.csv\")\n",
        "val_data = pd.read_csv(\"val_QC_data.csv\")\n",
        "test_data = pd.read_csv(\"test_QC_data.csv\")\n",
        "\n",
        "train_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  questionID originalQuestionID  totalPossiblePoint AnswerKey  \\\n",
              "0            VASoL_2008_3_34                 34                   1         C   \n",
              "1              MCAS_2015_8_6                  6                   1         B   \n",
              "2          Mercury_SC_417677             417677                   1         B   \n",
              "3            Mercury_7230423            7230423                   1         A   \n",
              "4      NYSEDREGENTS_2007_8_6                  6                   1         2   \n",
              "...                      ...                ...                 ...       ...   \n",
              "5592          Mercury_402502             402502                   1         D   \n",
              "5593          MCAS_2006_9_20                 20                   1         B   \n",
              "5594  NYSEDREGENTS_2013_8_35                 35                   1         4   \n",
              "5595         Mercury_7082670            7082670                   1         C   \n",
              "5596         Mercury_7004970            7004970                   1         C   \n",
              "\n",
              "      isMultipleChoiceQuestion  includesDiagram  \\\n",
              "0                            1                0   \n",
              "1                            1                0   \n",
              "2                            1                0   \n",
              "3                            1                0   \n",
              "4                            1                0   \n",
              "...                        ...              ...   \n",
              "5592                         1                0   \n",
              "5593                         1                0   \n",
              "5594                         1                0   \n",
              "5595                         1                0   \n",
              "5596                         1                0   \n",
              "\n",
              "                                      examName  grade  year  \\\n",
              "0     Virginia Standards of Learning - Science      3  2008   \n",
              "1                                         MCAS      8  2015   \n",
              "2                                      Mercury      4  2015   \n",
              "3                                      Mercury      9  2015   \n",
              "4                                 NYSEDREGENTS      8  2007   \n",
              "...                                        ...    ...   ...   \n",
              "5592                                   Mercury      8  2015   \n",
              "5593                                      MCAS      9  2006   \n",
              "5594                              NYSEDREGENTS      8  2013   \n",
              "5595                                   Mercury      7  2015   \n",
              "5596                                   Mercury      8  2015   \n",
              "\n",
              "                                                QCLabel  \\\n",
              "0                     matter_properties of objects_TEXT   \n",
              "1                            celestial_FEATURES_STELLAR   \n",
              "2                                  energy_LIGHT_REFLECT   \n",
              "3                                LIFE_EXTINCTION_MASSEX   \n",
              "4     Life_functions_features and functions_CELLBIO_...   \n",
              "...                                                 ...   \n",
              "5592                    matter_chemistry_periodic table   \n",
              "5593                                       FOR_MOMENTUM   \n",
              "5594  Life_functions_features and functions_PLANT_PH...   \n",
              "5595              energy_LIGHT_electromagnetic spectrum   \n",
              "5596        Life_reproduction_DNA inheritance_DOMRECESS   \n",
              "\n",
              "                                               Question  subject category  \\\n",
              "0     A student is asked to bring something that fee...      NaN    Train   \n",
              "1     Which of the following statements best describ...      NaN     Test   \n",
              "2     A polished metal ball looks very shiny and bri...      NaN     Test   \n",
              "3     Which was a main force driving extensive speci...      NaN     Test   \n",
              "4     Compared to the amount of hereditary informati...      NaN    Train   \n",
              "...                                                 ...      ...      ...   \n",
              "5592  According to the periodic table, argon is foun...      NaN     Test   \n",
              "5593  Which of the following has the least momentum?...      NaN    Train   \n",
              "5594  The amount of which greenhouse gas in the air ...      NaN     Test   \n",
              "5595  The visible light spectrum can be subdivided a...      NaN     Test   \n",
              "5596  A scientist crosses a red-flowered plant with ...      NaN    Train   \n",
              "\n",
              "           fold  \n",
              "0          Easy  \n",
              "1          Easy  \n",
              "2     Challenge  \n",
              "3          Easy  \n",
              "4     Challenge  \n",
              "...         ...  \n",
              "5592  Challenge  \n",
              "5593  Challenge  \n",
              "5594       Easy  \n",
              "5595       Easy  \n",
              "5596       Easy  \n",
              "\n",
              "[5597 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-508d6e39-76f8-43d9-a604-8733868f69f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>grade</th>\n",
              "      <th>year</th>\n",
              "      <th>QCLabel</th>\n",
              "      <th>Question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VASoL_2008_3_34</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Virginia Standards of Learning - Science</td>\n",
              "      <td>3</td>\n",
              "      <td>2008</td>\n",
              "      <td>matter_properties of objects_TEXT</td>\n",
              "      <td>A student is asked to bring something that fee...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MCAS_2015_8_6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>celestial_FEATURES_STELLAR</td>\n",
              "      <td>Which of the following statements best describ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mercury_SC_417677</td>\n",
              "      <td>417677</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>4</td>\n",
              "      <td>2015</td>\n",
              "      <td>energy_LIGHT_REFLECT</td>\n",
              "      <td>A polished metal ball looks very shiny and bri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mercury_7230423</td>\n",
              "      <td>7230423</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>LIFE_EXTINCTION_MASSEX</td>\n",
              "      <td>Which was a main force driving extensive speci...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NYSEDREGENTS_2007_8_6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>8</td>\n",
              "      <td>2007</td>\n",
              "      <td>Life_functions_features and functions_CELLBIO_...</td>\n",
              "      <td>Compared to the amount of hereditary informati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5592</th>\n",
              "      <td>Mercury_402502</td>\n",
              "      <td>402502</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>matter_chemistry_periodic table</td>\n",
              "      <td>According to the periodic table, argon is foun...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5593</th>\n",
              "      <td>MCAS_2006_9_20</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>9</td>\n",
              "      <td>2006</td>\n",
              "      <td>FOR_MOMENTUM</td>\n",
              "      <td>Which of the following has the least momentum?...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5594</th>\n",
              "      <td>NYSEDREGENTS_2013_8_35</td>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>8</td>\n",
              "      <td>2013</td>\n",
              "      <td>Life_functions_features and functions_PLANT_PH...</td>\n",
              "      <td>The amount of which greenhouse gas in the air ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5595</th>\n",
              "      <td>Mercury_7082670</td>\n",
              "      <td>7082670</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>energy_LIGHT_electromagnetic spectrum</td>\n",
              "      <td>The visible light spectrum can be subdivided a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5596</th>\n",
              "      <td>Mercury_7004970</td>\n",
              "      <td>7004970</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Life_reproduction_DNA inheritance_DOMRECESS</td>\n",
              "      <td>A scientist crosses a red-flowered plant with ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5597 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-508d6e39-76f8-43d9-a604-8733868f69f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-508d6e39-76f8-43d9-a604-8733868f69f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-508d6e39-76f8-43d9-a604-8733868f69f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQhO6qqt6lge"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD4IAgG1XQGp"
      },
      "source": [
        "import re\n",
        "def clean_sentence(question):\n",
        "  # print(question)\n",
        "  question = re.sub('<[^>]*>', ' ',question)\n",
        "  question = re.sub(' +', ' ', question)\n",
        "  question = re.sub('\\xa0','',question)\n",
        "  question = question.rstrip()\n",
        "  question = re.sub('nan','',question)\n",
        "  question = re.sub(u'\\u2004','',question)\n",
        "  question = re.sub(u'\\u2009','',question)\n",
        "\n",
        "  # question = question.decode(\"utf-8\")\n",
        "  # question = question.replace(u'\\u200\\d*','').encode(\"utf-8\")\n",
        "  question = re.sub('&nbsp','',question)\n",
        "  question = re.sub('&ndash','',question)\n",
        "  question = re.sub('\\r','',question)\n",
        "  question = re.sub('\\t','',question)\n",
        "  question = re.sub('\\n',' ',question)\n",
        "\n",
        "  question = re.sub('MathType@.*','',question)\n",
        "  question = re.sub('&thinsp','',question)\n",
        "  question = re.sub('&times','',question)\n",
        "  question = re.sub('\\u200b','',question)\n",
        "  question = re.sub('&rarr;;;','',question)\n",
        "\n",
        "  return question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNrGNk8f3kgh"
      },
      "source": [
        "# final_data_1 = final_data.loc[0:71003,:]\n",
        "# final_data_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrS5sxE3kgk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "346b92be09eb417981366479c69882ba",
            "42b4c9138372408faea573bc15b647da",
            "871916844fa74f7b8e630392f2de16d2",
            "dcc905a354b442c7b0328d714f151c5f",
            "f104cccf33f443ac92aab685259ed124",
            "f644b0e48b34460292a002037faf5975",
            "3746db1fa8104e09b5b753c04cf47e5c",
            "8c63fd59f2354fa5b06e9fbbcdd8c40c",
            "972e28e267cb44d7a7be5c90e8b98a89",
            "f1eca407adcd43c19e08af1040b6cadd",
            "e88f57a808444be5a9b56c6f2fe4ccac"
          ]
        },
        "outputId": "7db81cd4-9e86-499c-efe4-385a4a6caef4"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "346b92be09eb417981366479c69882ba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgc72PQYV1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648aff7c-e3b7-4afa-a4c7-b9ffb4f2bebc"
      },
      "source": [
        "train_data[\"QCLabel\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "celestial_cycles                             106\n",
              "matter_chemistry_periodic table               85\n",
              "matter_chemistry_atomic                       79\n",
              "matter_CHANGES_CHEMICAL                       72\n",
              "science_INFERENCE_observation                 69\n",
              "                                            ... \n",
              "EARTH_GEO_HISTORY                              1\n",
              "science_INFERENCE_INFERENCE                    1\n",
              "FOR_AIRRESISTANCE                              1\n",
              "matter_measurement_UNIT_PH                     1\n",
              "energy_ELEC_ELECTROMAGNETS_energy_devices      1\n",
              "Name: QCLabel, Length: 416, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3lYOb2K3kgy"
      },
      "source": [
        "\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# LE = LabelEncoder()\n",
        "# final_data['label'] = LE.fit_transform(final_data['board_syllabus'])\n",
        "# final_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp64MkNB3kg1"
      },
      "source": [
        "# def get_labels(prediction):\n",
        "#     predicted_label =  LE.inverse_transform([prediction])\n",
        "#     return predicted_label[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPgTmJPS3kg4"
      },
      "source": [
        "# get_labels(330)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrQ1BBx0vWUQ"
      },
      "source": [
        "# train_data = pd.concat([train_data,val_data])\n",
        "# train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuhr8z0tcd_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4788a41c-2648-48f0-dde7-79bb5deee607"
      },
      "source": [
        "val_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            questionID originalQuestionID  totalPossiblePoint AnswerKey  \\\n",
              "0    MCAS_2006_9_30-v1                 30                   1         D   \n",
              "1    Mercury_SC_401144             401144                   1         B   \n",
              "2     NCEOGA_2013_8_46                 46                   1         A   \n",
              "3       Mercury_417146             417146                   1         A   \n",
              "4      Mercury_7283833            7283833                   1         B   \n",
              "..                 ...                ...                 ...       ...   \n",
              "773    Mercury_7128853            7128853                   1         A   \n",
              "774    Mercury_7245858            7245858                   1         A   \n",
              "775     Mercury_417462             417462                   1         C   \n",
              "776    Mercury_7044065            7044065                   1         D   \n",
              "777    Mercury_7094238            7094238                   1         C   \n",
              "\n",
              "     isMultipleChoiceQuestion  includesDiagram  \\\n",
              "0                           1                0   \n",
              "1                           1                0   \n",
              "2                           1                0   \n",
              "3                           1                0   \n",
              "4                           1                0   \n",
              "..                        ...              ...   \n",
              "773                         1                0   \n",
              "774                         1                0   \n",
              "775                         1                0   \n",
              "776                         1                0   \n",
              "777                         1                0   \n",
              "\n",
              "                                         examName  grade  year  \\\n",
              "0                                            MCAS      9  2006   \n",
              "1                                         Mercury      5  2015   \n",
              "2    North Carolina READY End-of-Grade Assessment      8  2013   \n",
              "3                                         Mercury      8  2015   \n",
              "4                                         Mercury      8  2015   \n",
              "..                                            ...    ...   ...   \n",
              "773                                       Mercury      8  2015   \n",
              "774                                       Mercury      7  2015   \n",
              "775                                       Mercury      7  2015   \n",
              "776                                       Mercury      8  2015   \n",
              "777                                       Mercury      7  2015   \n",
              "\n",
              "                                               QCLabel  \\\n",
              "0                              matter_CHANGES_PHYSICAL   \n",
              "1                                 EARTH_WEATHER_CLOUDS   \n",
              "2                                 EARTH_GEO_FORMATIONS   \n",
              "3             Life_interdependence_ecological features   \n",
              "4                LIFE_HEALTH_DIESEASE_PANDEMICEPIDEMIC   \n",
              "..                                                 ...   \n",
              "773                             energy_SOUND_AMPLITUDE   \n",
              "774                            celestial_SPACEEX_HUMAN   \n",
              "775  Life_functions_features and functions_PLANT_RE...   \n",
              "776             EARTH_human impacts_WHAT_air pollution   \n",
              "777                                              OTHER   \n",
              "\n",
              "                                              Question  subject category  \\\n",
              "0    Which of the following changes occurs as a sol...      NaN    Train   \n",
              "1    When water vapor rises and cools, the liquid w...      NaN    Train   \n",
              "2    Which best describes the characteristics of a ...      NaN      Dev   \n",
              "3    Most of the oxygen in the atmosphere is made b...      NaN      Dev   \n",
              "4    Which aspect of modern Life_could most likely ...      NaN     Test   \n",
              "..                                                 ...      ...      ...   \n",
              "773  As the loudness of a sound wave increases, whi...      NaN     Test   \n",
              "774  In the initial stages of manned space explorat...      NaN    Train   \n",
              "775  During a walk in the woods, Mandy finds a plan...      NaN     Test   \n",
              "776  What is the MAJOR cause of acid rain? (A) smel...      NaN     Test   \n",
              "777  Which invention would a culture living above t...      NaN    Train   \n",
              "\n",
              "          fold  \n",
              "0    Challenge  \n",
              "1         Easy  \n",
              "2         Easy  \n",
              "3    Challenge  \n",
              "4    Challenge  \n",
              "..         ...  \n",
              "773       Easy  \n",
              "774       Easy  \n",
              "775       Easy  \n",
              "776       Easy  \n",
              "777  Challenge  \n",
              "\n",
              "[778 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25536605-1b3e-4aa1-bc27-cc3efa570f12\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>grade</th>\n",
              "      <th>year</th>\n",
              "      <th>QCLabel</th>\n",
              "      <th>Question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MCAS_2006_9_30-v1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>9</td>\n",
              "      <td>2006</td>\n",
              "      <td>matter_CHANGES_PHYSICAL</td>\n",
              "      <td>Which of the following changes occurs as a sol...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mercury_SC_401144</td>\n",
              "      <td>401144</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "      <td>EARTH_WEATHER_CLOUDS</td>\n",
              "      <td>When water vapor rises and cools, the liquid w...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NCEOGA_2013_8_46</td>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>North Carolina READY End-of-Grade Assessment</td>\n",
              "      <td>8</td>\n",
              "      <td>2013</td>\n",
              "      <td>EARTH_GEO_FORMATIONS</td>\n",
              "      <td>Which best describes the characteristics of a ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dev</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mercury_417146</td>\n",
              "      <td>417146</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Life_interdependence_ecological features</td>\n",
              "      <td>Most of the oxygen in the atmosphere is made b...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dev</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mercury_7283833</td>\n",
              "      <td>7283833</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>LIFE_HEALTH_DIESEASE_PANDEMICEPIDEMIC</td>\n",
              "      <td>Which aspect of modern Life_could most likely ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>Mercury_7128853</td>\n",
              "      <td>7128853</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>energy_SOUND_AMPLITUDE</td>\n",
              "      <td>As the loudness of a sound wave increases, whi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>Mercury_7245858</td>\n",
              "      <td>7245858</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>celestial_SPACEEX_HUMAN</td>\n",
              "      <td>In the initial stages of manned space explorat...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>Mercury_417462</td>\n",
              "      <td>417462</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Life_functions_features and functions_PLANT_RE...</td>\n",
              "      <td>During a walk in the woods, Mandy finds a plan...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>Mercury_7044065</td>\n",
              "      <td>7044065</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>EARTH_human impacts_WHAT_air pollution</td>\n",
              "      <td>What is the MAJOR cause of acid rain? (A) smel...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "      <td>Easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>Mercury_7094238</td>\n",
              "      <td>7094238</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>Which invention would a culture living above t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Train</td>\n",
              "      <td>Challenge</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>778 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25536605-1b3e-4aa1-bc27-cc3efa570f12')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25536605-1b3e-4aa1-bc27-cc3efa570f12 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25536605-1b3e-4aa1-bc27-cc3efa570f12');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijn1nIpByb3e"
      },
      "source": [
        "train_features = train_data[\"Question\"]\n",
        "test_features = test_data[\"Question\"]\n",
        "train_labels = train_data[\"QCLabel\"]\n",
        "test_labels = test_data[\"QCLabel\"]\n",
        "val_features = val_data[\"Question\"]\n",
        "val_labels = val_data[\"QCLabel\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prM_km_83khD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5968c120-e654-4376-996b-21cd2e421d39"
      },
      "source": [
        "train_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "celestial_cycles                             106\n",
              "matter_chemistry_periodic table               85\n",
              "matter_chemistry_atomic                       79\n",
              "matter_CHANGES_CHEMICAL                       72\n",
              "science_INFERENCE_observation                 69\n",
              "                                            ... \n",
              "EARTH_GEO_HISTORY                              1\n",
              "science_INFERENCE_INFERENCE                    1\n",
              "FOR_AIRRESISTANCE                              1\n",
              "matter_measurement_UNIT_PH                     1\n",
              "energy_ELEC_ELECTROMAGNETS_energy_devices      1\n",
              "Name: QCLabel, Length: 416, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhPstXJ03oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3eab2b-1237-4e40-97b5-f775549deb63"
      },
      "source": [
        "test_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "celestial_cycles                                          26\n",
              "Life_reproduction_DNA inheritance_inheritance             26\n",
              "Life_functions_features and functions_PLANT_PHOTOSYNTH    22\n",
              "matter_chemistry_atomic                                   21\n",
              "science_INFERENCE_experiment design                       19\n",
              "                                                          ..\n",
              "energy_LIGHT_REFLECT                                       1\n",
              "Life_cycle_animal cycle_BIRD                               1\n",
              "LIFE_HEALTH_DIESEASE_PREVENTION                            1\n",
              "matter_measurement_UNIT_MASS                               1\n",
              "matter_STATES_SOLID                                        1\n",
              "Name: QCLabel, Length: 352, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyM7gqv3khI"
      },
      "source": [
        "\n",
        "question_answer = train_features.values\n",
        "categories = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFkS_H_83khL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2566ba87-ecbc-4d08-aecf-fed95391a9ff"
      },
      "source": [
        "question_answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['A student is asked to bring something that feels rough to class. Which would be BEST for him to bring? (A) Pillow (B) Marble (C) Sandpaper (D) Trading card',\n",
              "       'Which of the following statements best describes the role of gravity in the formation of stars? (A) Gravity converts solid matter into gases and light energy. (B) Gravity causes gases and dust particles to condense into spheres. (C) Gravity cools gases and liquids until they become one solid mass. (D) Gravity pushes rocks and dust particles outward from a dense center.',\n",
              "       'A polished metal ball looks very shiny and bright on a sunny day. What makes the ball look shiny? (A) The ball makes light. (B) The ball reflects light. (C) The ball absorbs light and then releases it. (D) The ball absorbs light and keeps it inside.',\n",
              "       ...,\n",
              "       'The amount of which greenhouse gas in the air will increase the most if large forests are cut down to be used for building materials without planting new trees in their place? (1) ozone (2) methane (3) water vapor (4) carbon dioxide',\n",
              "       'The visible light spectrum can be subdivided according to (A) the types of waves. (B) the sizes of particles. (C) a range of colors. (D) a type of energy.',\n",
              "       'A scientist crosses a red-flowered plant with a white-flowered plant, and all offspring have red flowers. What will most likely result if these red-flowered offspring are crossed with white-flowered plants? (A) All of the offspring will have red flowers. (B) All of the offspring will have white flowers. (C) The offspring will have either red or white flowers. (D) The offspring will have neither red nor white flowers.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ian7gSDE3khR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd4c6c7-9506-4cf0-f32e-1d793c40dd71"
      },
      "source": [
        "categories"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['matter_properties of objects_TEXT', 'celestial_FEATURES_STELLAR',\n",
              "       'energy_LIGHT_REFLECT', ...,\n",
              "       'Life_functions_features and functions_PLANT_PHOTOSYNTH',\n",
              "       'energy_LIGHT_electromagnetic spectrum',\n",
              "       'Life_reproduction_DNA inheritance_DOMRECESS'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fepGiggpqOQx"
      },
      "source": [
        "# val_features = test_features.values\n",
        "# val_labels = test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN1zRXMOXwLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28b0c62-a376-4d02-9576-1a222dcadb61"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "!pip install inflection\n",
        "\n",
        "from bokeh.io import output_file, output_notebook, show\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.util.hex import hexbin\n",
        "from bokeh.models import HoverTool\n",
        "from bokeh import colors\n",
        "import inflection\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer()\n",
        "from gzip import open as gopen\n",
        "from pandas.core.common import flatten\n",
        "import gensim.models.poincare as poincare\n",
        "def get_cleaned_taxonomy(taxonomy):\n",
        "  cleaned_taxonomy = []\n",
        "  for value in taxonomy:\n",
        "      value = ' '.join(value.lower().split(\"_\"))\n",
        "      # taxonomy_words = [inflection.singularize(val)  for token in value for val in token.split(\" \") if val.isalpha()]\n",
        "      cleaned_taxonomy.append( value )\n",
        "  return cleaned_taxonomy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting inflection\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: inflection\n",
            "Successfully installed inflection-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPAl0TNuX6mx"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "\n",
        "poincare_emb_data = get_cleaned_taxonomy(categories)\n",
        "poincare_val = get_cleaned_taxonomy(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aedZzkBsqEeK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ce3dc47-0cc6-441a-8c39-1eddab022b0c"
      },
      "source": [
        "poincare_emb_data[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'energy light reflect'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ocuHxzCy16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88113792-6bcf-4bd9-df4c-76d4dd5d3051"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjDKIFSENir_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616ad6e7-e69c-42dc-9fce-9a931efcf4ac"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.24G/1.24G [00:57<00:00, 21.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
      ],
      "metadata": {
        "id": "0XlCt0Oicof1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdBbsrO9zp2p"
      },
      "source": [
        "# taxonomy_vectors = []\n",
        "taxonomy_vectors = model.encode(poincare_emb_data)\n",
        "# taxonomy_vectors = np.vstack(taxonomy_vectors)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJUSQmOq7v8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c185b4b-a5c4-4c3c-b4a0-183ce2bda572"
      },
      "source": [
        "taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
        "taxonomy_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5597, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTlCYX9Q34JG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387da9c8-988e-419e-bffa-c9334908c36e"
      },
      "source": [
        "# taxonomy_vectors_val = []\n",
        "# for feature in poincare_val:\n",
        "taxonomy_vectors_val = model.encode(poincare_val)\n",
        "taxonomy_vectors_val = np.vstack(taxonomy_vectors_val)\n",
        "taxonomy_vectors_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(778, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p834oM1Pzzu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c323f843-94d0-4dab-a07e-db994b6e5abf"
      },
      "source": [
        "set(train_data[\"Question\"].values).intersection(set(test_data[\"Question\"].values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ZeuHc63khU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858aa400-f21f-4527-8065-4d8a718ba7d7"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in question_answer:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  A student is asked to bring something that feels rough to class. Which would be BEST for him to bring? (A) Pillow (B) Marble (C) Sandpaper (D) Trading card\n",
            "Token IDs: tensor([  101,  1037,  3076,  2003,  2356,  2000,  3288,  2242,  2008,  5683,\n",
            "         5931,  2000,  2465,  1012,  2029,  2052,  2022,  2190,  2005,  2032,\n",
            "         2000,  3288,  1029,  1006,  1037,  1007, 10005,  1006,  1038,  1007,\n",
            "         7720,  1006,  1039,  1007,  5472, 23298,  1006,  1040,  1007,  6202,\n",
            "         4003,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjkhiN3pAfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583319ab-f271-4b73-f85c-e8dd81dcc27d"
      },
      "source": [
        "input_ids_val = []\n",
        "attention_masks_val = []\n",
        "\n",
        "for sent in val_features:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids_val.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
        "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  A student is asked to bring something that feels rough to class. Which would be BEST for him to bring? (A) Pillow (B) Marble (C) Sandpaper (D) Trading card\n",
            "Token IDs: tensor([  101,  1037,  3076,  2003,  2356,  2000,  3288,  2242,  2008,  5683,\n",
            "         5931,  2000,  2465,  1012,  2029,  2052,  2022,  2190,  2005,  2032,\n",
            "         2000,  3288,  1029,  1006,  1037,  1007, 10005,  1006,  1038,  1007,\n",
            "         7720,  1006,  1039,  1007,  5472, 23298,  1006,  1040,  1007,  6202,\n",
            "         4003,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDW74Ny3khj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30adcdc-f8fd-4198-ca0b-63f29ea32597"
      },
      "source": [
        "num_classes = len(list(set(categories)))\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "416"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmaLk5Ab3khl"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "train_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "val_poincare_tensor = torch.tensor(taxonomy_vectors_val,dtype=torch.float)\n",
        "\n",
        "val_dataset = TensorDataset(input_ids_val,attention_masks_val,val_poincare_tensor)\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, train_poincare_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_lTinod3kho"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vduf9fOMviK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9262747-561f-46d8-ea86-8158b9a42a4d"
      },
      "source": [
        "# !pip install transformers==2.8.0\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H2wp8WlEi9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d090fb-78fd-4d61-c13c-cb276b019abf"
      },
      "source": [
        "set(question_answer).intersection(set(test_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6jdKp0uhO3"
      },
      "source": [
        "\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "\n",
        "from tqdm import tqdm\n",
        "import geoopt\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "cos_label = nn.CosineSimilarity(dim=1, eps=1e-5)\n",
        "\n",
        "dist = torch.nn.PairwiseDistance(p=2.0, eps=1e-06)\n",
        "nn.PairwiseDistance(p=2)\n",
        "class MHSA(nn.Module):\n",
        "  def __init__(self,\n",
        "         emb_dim,\n",
        "         kqv_dim,\n",
        "         num_heads=2):\n",
        "    super(MHSA, self).__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.kqv_dim = kqv_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.w_k = nn.Linear(emb_dim, kqv_dim * num_heads, bias=False)\n",
        "    self.w_q = nn.Linear(emb_dim, kqv_dim * num_heads, bias=False)\n",
        "    self.w_v = nn.Linear(emb_dim, kqv_dim * num_heads, bias=False)\n",
        "    self.w_out = nn.Linear(kqv_dim * num_heads, emb_dim)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    # print(\"query\",query.shape)\n",
        "    b, t = query.shape\n",
        "    e = self.kqv_dim\n",
        "    h = self.num_heads\n",
        "    keys = self.w_k(key).view(b, h, e)\n",
        "    values = self.w_v(value).view(b, h, e)\n",
        "    queries = self.w_q(query).view(b, h, e)\n",
        "\n",
        "    # keys = keys.transpose(2, 1)\n",
        "    # queries = queries.transpose(2, 1)\n",
        "    # values = values.transpose(2, 1)\n",
        "\n",
        "    dot = queries @ keys.transpose(2, 1)  #(b*h*e) @ (b*e*h)\n",
        "    dot = dot / np.sqrt(e)  # (b*h*h)\n",
        "    dot = F.softmax(dot, dim=2)\n",
        "\n",
        "    out = dot @ values   # (b*h*h) @ (b*h*e) = (b*h*e)\n",
        "    out = out.contiguous().view(b, h * e)\n",
        "    out = self.w_out(out)\n",
        "    return out\n",
        "# Neural Classifierwork\n",
        "\n",
        "# Discussion TODOS\n",
        "# try hierarhical interaction (TODO)\n",
        "\n",
        "# try bringing in modalities (image, or video)\n",
        "\n",
        "# Go from classical algorithm -> deep learning\n",
        "\n",
        "class MulticlassClassifier(nn.Module):\n",
        "    def __init__(self,bert_model_path):\n",
        "        super(MulticlassClassifier,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=False)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(768, 1024)\n",
        "        self.fc2 = nn.Linear(576, 1024)\n",
        "        self.fc3 = nn.Linear(2048,1024)\n",
        "        self.multi_head_attention = MHSA(1024, 512,8)\n",
        "        self.multihead_attn = torch.nn.MultiheadAttention(embed_dim = 1024,  num_heads = 16, batch_first=True)\n",
        "\n",
        "\n",
        "    def forward(self,tokens,masks, targets=None, skip_attention=False):\n",
        "        # print(\"tokens\", tokens.shape)\n",
        "        outputs = self.bert(tokens, attention_mask=masks)[2]\n",
        "        # outputs[2] = outputs[2].permute(0,2,1)\n",
        "        output_1 = outputs[-1].permute(1,0,2)\n",
        "        # print(outputs[1].shape,outputs[0].shape)\n",
        "        output_1 = torch.mean(output_1, dim=0)\n",
        "        # output_2 = outputs[-2].permute(1,0,2)\n",
        "        # output_2 = torch.mean(output_2, dim=0)\n",
        "        # print(\"output_2\", output_2.shape, output_1.shape)\n",
        "        pooled_output = outputs[-1] #output_1 # torch.cat((output_1, output_2), dim=1)\n",
        "        # print(\"pooled_output\", pooled_output.shape)\n",
        "        x = self.fc1(pooled_output)\n",
        "        # print(\"x shape\",x.shape)\n",
        "        targets_curr_batch = []\n",
        "        for index_1, input_x in enumerate(x):\n",
        "            # print(input_x.shape, torch.mean(input_x,dim=0).shape)\n",
        "            distance = cos_label(torch.mean(input_x,dim=0).reshape(1,-1), unique_poincare_tensor)\n",
        "            distances,indices = torch.topk(distance,1,largest=True)\n",
        "\n",
        "            target_distances = (F.normalize(unique_poincare_tensor[indices],p=2,dim=1) - F.normalize(unique_poincare_tensor,p=2,dim=1)).pow(2).sum(1) #cos_label(unique_poincare_tensor[indices].reshape(1,-1), unique_poincare_tensor)\n",
        "            distances,indices = torch.topk(target_distances,5,largest=False)\n",
        "            targets_curr_batch.append(unique_poincare_tensor[indices].reshape(1,5,1024))\n",
        "            # print(\"here\")\n",
        "        # print(len(targets_curr_batch))\n",
        "        targets_batch = torch.cat(targets_curr_batch, dim=0)\n",
        "        # print(\"targets_batch\",targets_batch.shape)\n",
        "        attn_output, attn_output_weights = self.multihead_attn(targets_batch, x, x)\n",
        "        attn_out = torch.mean(attn_output,dim=1)\n",
        "        # x = torch.cat((torch.mean(x,dim=1),attn_out),1)\n",
        "\n",
        "\n",
        "        # print(\"X shape\",x.shape)\n",
        "        return attn_out\n",
        "\n",
        "class MyHingeLoss(torch.nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(MyHingeLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    # def forward_val(self, output, target):\n",
        "    #     cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "    #     loss = 0\n",
        "    #     num_compare = 4\n",
        "    #     count = 0\n",
        "    #     for i in range(len(output)):\n",
        "    #         v_image = output[i]\n",
        "    #         t_label = target[i]\n",
        "    #         for j in range(num_compare):\n",
        "    #             if j != i:\n",
        "    #                 count += 1\n",
        "    #                 t_j = target[j]\n",
        "    #                 loss += torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
        "    #     return loss / count\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss=0\n",
        "        for i in range(len(output)):\n",
        "            v_image = F.normalize(output[i],p=2,dim=0)\n",
        "            t_label = F.normalize(target[i],p=2,dim=0)\n",
        "            # j = randint(0, len(output)-1)\n",
        "            # while j == i:\n",
        "            #     j = randint(0, len(output)-1)\n",
        "            distance = cos_label(t_label, target)\n",
        "            # print(distance.shape)\n",
        "            delta = min(len(target)-2,8)\n",
        "            distances,indices = torch.topk(distance,len(target)-delta,largest=True)\n",
        "            # print(indices)\n",
        "            # index_target = random.choice(indices)\n",
        "            # while index_target == i:\n",
        "            #     # print(\"here***\", index_target,i)\n",
        "            #     index_target = random.choice(indices)\n",
        "            count = 0\n",
        "            for index_target in indices:\n",
        "                if index_target!=i:\n",
        "                    count=count+1\n",
        "                    t_j = F.normalize(target[index_target],p=2,dim=0)\n",
        "                    loss+= torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
        "            # print(\"count\",count)\n",
        "        return loss / (len(output) * count)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHYyjxMDIx2U"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KLfgh1K44qY"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2tmAMlw3khr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14218acb-d807-4e92-fd80-d7075ef9012b"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "\n",
        "# Loads BertModel, the pretrained BERT model with a single \n",
        "model = MulticlassClassifier('bert-base-uncased')\n",
        "# model.load_state_dict(torch.load('model_euclidean_SENT_BERT_cos_1/model_weights'))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MulticlassClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=576, out_features=1024, bias=True)\n",
              "  (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "  (multi_head_attention): MHSA(\n",
              "    (w_k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "    (w_q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "    (w_v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "    (w_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "  )\n",
              "  (multihead_attn): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4paz_8iTZ9o"
      },
      "source": [
        "# mobius_params = []\n",
        "# bert_params = []\n",
        "\n",
        "# def mobius_params():\n",
        "#   for param in model.named_parameters():\n",
        "#     if 'fc' in param[0]:\n",
        "#       yield param[1]\n",
        "# def bert_params():\n",
        "#   for param in model.named_parameters():\n",
        "#     if 'bert' in param[0]:\n",
        "#       yield param[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awQ2Y9Jb3kht"
      },
      "source": [
        "optimizer_1 = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "# optimizer_2 = radam_.RiemannianAdam(mobius_params(), lr=0.01, stabilize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ys-M4-e3khv"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrYqErOD3khx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea720840-9643-4051-b995-028640cb8f51"
      },
      "source": [
        "len(train_dataloader) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWVSE9LM3kh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb608789-3636-4db6-82d6-f5451b577a01"
      },
      "source": [
        "1935 * 32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61920"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcvxVVi63kh3"
      },
      "source": [
        "scheduler = get_linear_schedule_with_warmup(optimizer_1, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUw3zm6g3kh5"
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Function to calculate the accuracy of our predictions vs labels\n",
        "# def flat_accuracy(preds, labels):\n",
        "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "#     labels_flat = labels.flatten()\n",
        "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta6zfUTa3kh7"
      },
      "source": [
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFq9gd5kQSHb"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsInxVoqbsFW"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di68jXK5WaYr"
      },
      "source": [
        "criterion = MyHingeLoss(0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRcFqo9Ya6h6"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "train_labels = list(set(train_data[\"QCLabel\"].values))\n",
        "emb_data_train = get_cleaned_taxonomy(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e002c69-9a6d-4bac-a0ed-6aa706caeb7d",
        "id": "R7YLX-ZCa6h7"
      },
      "source": [
        "# taxonomy_vectors = []\"\"\n",
        "taxonomy_vectors = sent_model.encode(emb_data_train)\n",
        "taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
        "taxonomy_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQJ8KR5na6h7"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "unique_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_poincare_tensor = unique_poincare_tensor.to(device)"
      ],
      "metadata": {
        "id": "vFKXxjBq7u66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LhAy2hZ3kh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1efb06c-6d78-44f2-cc88-05a3abb3833b"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import f1_score\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "early_stopping = EarlyStopping(patience=6, verbose=True)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "\n",
        "        model.zero_grad() \n",
        "        optimizer_1.zero_grad()       \n",
        "\n",
        "        logits = model(b_input_ids, \n",
        "                             b_input_mask)\n",
        "        \n",
        "        loss = criterion.forward(logits,b_labels)\n",
        "\n",
        "  \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer_1.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_f1 = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "\n",
        "          logits = model(b_input_ids, \n",
        "                              b_input_mask)\n",
        "          \n",
        "        loss = criterion(logits,b_labels)\n",
        "\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy().round()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        # total_eval_f1 += f1_score(label_ids,logits, average='macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    # avg_val_accuracy = total_eval_f1 / len(validation_dataloader)\n",
        "    # print(\"  f1score: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    early_stopping(avg_val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break  \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    output_dir = 'model_euclidean_SENT_BERT_cos_QC_attention_V3/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
        "\n",
        "    !rm -rf \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_SENT_BERT_cos_QC_attention_V3\"\n",
        "    !mv model_euclidean_SENT_BERT_cos_QC_attention_V3 \"/content/drive/My Drive/research_lo_content_taxonomy_classification/\"\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:15.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:31.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:47.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:02.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:05:30\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (inf --> 0.011709).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:05:33\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.011709 --> 0.009641).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.009641 --> 0.008144).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:51.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:07.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:37\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.008144 --> 0.007612).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:06.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:34\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.007612 --> 0.007335).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:55.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:10.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:38\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.007335 --> 0.007107).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.007107 --> 0.006997).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:50.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:35\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:35.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:50.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:33\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.006997 --> 0.006788).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.006788 --> 0.006664).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.006664 --> 0.006626).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 2 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 14 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:04.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:32\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.006626 --> 0.006582).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:20\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 15 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:50.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:35\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 16 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:33\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 2 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 17 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:32.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:06.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:35\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.006582 --> 0.006547).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 18 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:50.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:33\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 19 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:06.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:34\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 2 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 20 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:06.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:34\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 3 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 21 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:18.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:51.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:08.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:36\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 4 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 22 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:51.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:06.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:34\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (0.006547 --> 0.006529).  Saving model ...\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 23 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:16.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:49.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:06.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:34\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 24 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:50.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:33\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 2 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:19\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 25 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:15.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:32.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:48.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:33\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 3 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 26 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:17.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:33.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:50.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:05.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:35\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 4 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:22\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 27 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:21.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:36.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:54.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:09.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:39\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 5 out of 6\n",
            "  Validation Loss: 0.01\n",
            "  Validation took: 0:00:17\n",
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n",
            "\n",
            "======== Epoch 28 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    175.    Elapsed: 0:01:15.\n",
            "  Batch    80  of    175.    Elapsed: 0:02:41.\n",
            "  Batch   120  of    175.    Elapsed: 0:03:56.\n",
            "  Batch   160  of    175.    Elapsed: 0:05:14.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:41\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 6 out of 6\n",
            "Early stopping\n",
            "\n",
            "Training complete!\n",
            "Total training took 2:46:44 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDWFZVminDiI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "outputId": "97aa6d42-5227-4e91-80a8-b48a92075e64"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss Training Time Validation Time\n",
              "epoch                                                          \n",
              "1           2.27e-02     1.17e-02       0:05:30         0:00:19\n",
              "2           7.61e-03     9.64e-03       0:05:33         0:00:19\n",
              "3           5.03e-03     8.14e-03       0:05:32         0:00:19\n",
              "4           3.88e-03     7.61e-03       0:05:37         0:00:19\n",
              "5           3.24e-03     7.33e-03       0:05:34         0:00:19\n",
              "6           2.69e-03     7.11e-03       0:05:38         0:00:19\n",
              "7           2.33e-03     7.00e-03       0:05:32         0:00:19\n",
              "8           2.27e-03     7.10e-03       0:05:35         0:00:17\n",
              "9           1.97e-03     6.79e-03       0:05:33         0:00:19\n",
              "10          1.78e-03     6.66e-03       0:05:32         0:00:19\n",
              "11          1.63e-03     6.63e-03       0:05:32         0:00:19\n",
              "12          1.58e-03     6.63e-03       0:05:32         0:00:17\n",
              "13          1.47e-03     6.68e-03       0:05:32         0:00:17\n",
              "14          1.41e-03     6.58e-03       0:05:32         0:00:20\n",
              "15          1.42e-03     6.70e-03       0:05:35         0:00:17\n",
              "16          1.33e-03     7.12e-03       0:05:33         0:00:17\n",
              "17          1.27e-03     6.55e-03       0:05:35         0:00:19\n",
              "18          1.12e-03     6.74e-03       0:05:33         0:00:17\n",
              "19          1.16e-03     6.61e-03       0:05:34         0:00:17\n",
              "20          1.18e-03     6.82e-03       0:05:34         0:00:17\n",
              "21          1.13e-03     6.87e-03       0:05:36         0:00:17\n",
              "22          1.08e-03     6.53e-03       0:05:34         0:00:19\n",
              "23          1.04e-03     6.98e-03       0:05:34         0:00:18\n",
              "24          1.10e-03     6.89e-03       0:05:33         0:00:19\n",
              "25          1.04e-03     6.98e-03       0:05:33         0:00:17\n",
              "26          1.01e-03     7.03e-03       0:05:35         0:00:22\n",
              "27          9.55e-04     7.09e-03       0:05:39         0:00:17"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb1167e2-b5d5-45ce-ac3f-962409a1ba4c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.27e-02</td>\n",
              "      <td>1.17e-02</td>\n",
              "      <td>0:05:30</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.61e-03</td>\n",
              "      <td>9.64e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.03e-03</td>\n",
              "      <td>8.14e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.88e-03</td>\n",
              "      <td>7.61e-03</td>\n",
              "      <td>0:05:37</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.24e-03</td>\n",
              "      <td>7.33e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.69e-03</td>\n",
              "      <td>7.11e-03</td>\n",
              "      <td>0:05:38</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.33e-03</td>\n",
              "      <td>7.00e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.27e-03</td>\n",
              "      <td>7.10e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.97e-03</td>\n",
              "      <td>6.79e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.78e-03</td>\n",
              "      <td>6.66e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.63e-03</td>\n",
              "      <td>6.63e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.58e-03</td>\n",
              "      <td>6.63e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.47e-03</td>\n",
              "      <td>6.68e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.41e-03</td>\n",
              "      <td>6.58e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.42e-03</td>\n",
              "      <td>6.70e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.33e-03</td>\n",
              "      <td>7.12e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.27e-03</td>\n",
              "      <td>6.55e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.12e-03</td>\n",
              "      <td>6.74e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.16e-03</td>\n",
              "      <td>6.61e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.18e-03</td>\n",
              "      <td>6.82e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.13e-03</td>\n",
              "      <td>6.87e-03</td>\n",
              "      <td>0:05:36</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.08e-03</td>\n",
              "      <td>6.53e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.04e-03</td>\n",
              "      <td>6.98e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.10e-03</td>\n",
              "      <td>6.89e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.04e-03</td>\n",
              "      <td>6.98e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.01e-03</td>\n",
              "      <td>7.03e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>9.55e-04</td>\n",
              "      <td>7.09e-03</td>\n",
              "      <td>0:05:39</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb1167e2-b5d5-45ce-ac3f-962409a1ba4c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb1167e2-b5d5-45ce-ac3f-962409a1ba4c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb1167e2-b5d5-45ce-ac3f-962409a1ba4c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RACcsko3kh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "outputId": "543862d2-253b-4af6-dee7-581da3395500"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss Training Time Validation Time\n",
              "epoch                                                          \n",
              "1           2.27e-02     1.17e-02       0:05:30         0:00:19\n",
              "2           7.61e-03     9.64e-03       0:05:33         0:00:19\n",
              "3           5.03e-03     8.14e-03       0:05:32         0:00:19\n",
              "4           3.88e-03     7.61e-03       0:05:37         0:00:19\n",
              "5           3.24e-03     7.33e-03       0:05:34         0:00:19\n",
              "6           2.69e-03     7.11e-03       0:05:38         0:00:19\n",
              "7           2.33e-03     7.00e-03       0:05:32         0:00:19\n",
              "8           2.27e-03     7.10e-03       0:05:35         0:00:17\n",
              "9           1.97e-03     6.79e-03       0:05:33         0:00:19\n",
              "10          1.78e-03     6.66e-03       0:05:32         0:00:19\n",
              "11          1.63e-03     6.63e-03       0:05:32         0:00:19\n",
              "12          1.58e-03     6.63e-03       0:05:32         0:00:17\n",
              "13          1.47e-03     6.68e-03       0:05:32         0:00:17\n",
              "14          1.41e-03     6.58e-03       0:05:32         0:00:20\n",
              "15          1.42e-03     6.70e-03       0:05:35         0:00:17\n",
              "16          1.33e-03     7.12e-03       0:05:33         0:00:17\n",
              "17          1.27e-03     6.55e-03       0:05:35         0:00:19\n",
              "18          1.12e-03     6.74e-03       0:05:33         0:00:17\n",
              "19          1.16e-03     6.61e-03       0:05:34         0:00:17\n",
              "20          1.18e-03     6.82e-03       0:05:34         0:00:17\n",
              "21          1.13e-03     6.87e-03       0:05:36         0:00:17\n",
              "22          1.08e-03     6.53e-03       0:05:34         0:00:19\n",
              "23          1.04e-03     6.98e-03       0:05:34         0:00:18\n",
              "24          1.10e-03     6.89e-03       0:05:33         0:00:19\n",
              "25          1.04e-03     6.98e-03       0:05:33         0:00:17\n",
              "26          1.01e-03     7.03e-03       0:05:35         0:00:22\n",
              "27          9.55e-04     7.09e-03       0:05:39         0:00:17"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce0c118d-8de4-4767-8e21-b9a3c44631b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.27e-02</td>\n",
              "      <td>1.17e-02</td>\n",
              "      <td>0:05:30</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.61e-03</td>\n",
              "      <td>9.64e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.03e-03</td>\n",
              "      <td>8.14e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.88e-03</td>\n",
              "      <td>7.61e-03</td>\n",
              "      <td>0:05:37</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.24e-03</td>\n",
              "      <td>7.33e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.69e-03</td>\n",
              "      <td>7.11e-03</td>\n",
              "      <td>0:05:38</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.33e-03</td>\n",
              "      <td>7.00e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.27e-03</td>\n",
              "      <td>7.10e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.97e-03</td>\n",
              "      <td>6.79e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.78e-03</td>\n",
              "      <td>6.66e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.63e-03</td>\n",
              "      <td>6.63e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.58e-03</td>\n",
              "      <td>6.63e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.47e-03</td>\n",
              "      <td>6.68e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.41e-03</td>\n",
              "      <td>6.58e-03</td>\n",
              "      <td>0:05:32</td>\n",
              "      <td>0:00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.42e-03</td>\n",
              "      <td>6.70e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.33e-03</td>\n",
              "      <td>7.12e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.27e-03</td>\n",
              "      <td>6.55e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.12e-03</td>\n",
              "      <td>6.74e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.16e-03</td>\n",
              "      <td>6.61e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.18e-03</td>\n",
              "      <td>6.82e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.13e-03</td>\n",
              "      <td>6.87e-03</td>\n",
              "      <td>0:05:36</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.08e-03</td>\n",
              "      <td>6.53e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.04e-03</td>\n",
              "      <td>6.98e-03</td>\n",
              "      <td>0:05:34</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.10e-03</td>\n",
              "      <td>6.89e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.04e-03</td>\n",
              "      <td>6.98e-03</td>\n",
              "      <td>0:05:33</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.01e-03</td>\n",
              "      <td>7.03e-03</td>\n",
              "      <td>0:05:35</td>\n",
              "      <td>0:00:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>9.55e-04</td>\n",
              "      <td>7.09e-03</td>\n",
              "      <td>0:05:39</td>\n",
              "      <td>0:00:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce0c118d-8de4-4767-8e21-b9a3c44631b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce0c118d-8de4-4767-8e21-b9a3c44631b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce0c118d-8de4-4767-8e21-b9a3c44631b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(10000000000000000000000000):\n",
        "    i=j"
      ],
      "metadata": {
        "id": "4nk_J8VYtknD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5TicdiP3kiC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbiTDpVv3kiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf430558-18db-48e3-df4d-420be1c71c05"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "output_dir = 'model_euclidean_SENT_BERT_cos_QC_attention_V3/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "\n",
        "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "# model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to model_euclidean_SENT_BERT_cos_QC_attention_V3/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model_euclidean_SENT_BERT_cos_QC_attention_V3/vocab.txt',\n",
              " 'model_euclidean_SENT_BERT_cos_QC_attention_V3/special_tokens_map.json',\n",
              " 'model_euclidean_SENT_BERT_cos_QC_attention_V3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U2UQ29a3kiI"
      },
      "source": [
        "# !pip install joblib\n",
        "# import joblib\n",
        "# joblib.dump(LE, \"label_encoder\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F5PxZm9vAOI"
      },
      "source": [
        "import json\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlb0IpVJO1XQ"
      },
      "source": [
        "# with open(os.path.join(output_dir, 'model_config.json'), 'w') as f:\n",
        "#     json.dump(model.config, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGY8vSDI6u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30bc7332-ca5f-4c3d-cee0-56afe7c2f524"
      },
      "source": [
        "!zip -r model_euclidean_SENT_BERT_cos_QC_attention_V3.zip model_euclidean_SENT_BERT_cos_QC_attention_V3\n",
        "# files.download('model_euclidean_1.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model_euclidean_SENT_BERT_cos_QC_attention_V3/ (stored 0%)\n",
            "  adding: model_euclidean_SENT_BERT_cos_QC_attention_V3/tokenizer_config.json (stored 0%)\n",
            "  adding: model_euclidean_SENT_BERT_cos_QC_attention_V3/vocab.txt (deflated 53%)\n",
            "  adding: model_euclidean_SENT_BERT_cos_QC_attention_V3/special_tokens_map.json (deflated 40%)\n",
            "  adding: model_euclidean_SENT_BERT_cos_QC_attention_V3/model_weights (deflated 8%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r  model_euclidean_SENT_BERT_cos_QC_attention_V3 \"/content/drive/MyDrive/research_lo_content_taxonomy_classification/\""
      ],
      "metadata": {
        "id": "A0L5bUuWInJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp   model_euclidean_SENT_BERT_cos_QC_attention_V3.zip \"/content/drive/MyDrive/research_lo_content_taxonomy_classification/\""
      ],
      "metadata": {
        "id": "_vygUpCvIuMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA1GBuz-03Sp"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKdRT1UUz4vC"
      },
      "source": [
        "# Run this cell and next if you want to test on LO zero shot setting If you want to test on ARC skip them.\n",
        "import pandas as pd\n",
        "lo_data = pd.read_csv(\"what_you_learnt_lo_labelled.csv\", delimiter=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2kDnBDF0I3e"
      },
      "source": [
        "test_features = lo_data[\"learning_objectives\"].values\n",
        "labels = lo_data[\"taxonomy\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvFDCDIxKDOf"
      },
      "source": [
        "# !zip -r label_encoder_categorized_reduced.zip label_encoder\n",
        "# files.download('label_encoder_categorized_reduced.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4178_yLFMWmx"
      },
      "source": [
        "# run from this cell to test on ARC data\n",
        "test_features = test_features.values\n",
        "labels = test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpmBJuIC2nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab238e91-8cdd-458b-e09b-0eb30ae537e0"
      },
      "source": [
        "test_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['A complete chemical equation represents the reactants products and their physical states symbolically.',\n",
              "       'A chemical equation is balanced so that the numbers of atoms of each type involved in a chemical reaction are the same on the reactant and product sides of theequation. Equations must always be balanced.',\n",
              "       'In a combination reaction two or more substances combine to form a new singlesubstance.',\n",
              "       'Decomposition reactions are opposite to combination reactions. In a decomposition reaction a single substance decomposes to give two or more substances.',\n",
              "       'Reactions in which heat is given out along with the products are called exothermic reactions.',\n",
              "       'Reactions in which energy is absorbed are known as endothermic reactions.',\n",
              "       'When an element displaces another element from its compound a displacement reaction occurs.',\n",
              "       'Precipitation reactions produce insoluble salts.',\n",
              "       'Two different atoms or groups of atoms (ions) are exchanged in double displacement reactions.',\n",
              "       'Reactions also involve the gain or loss of oxygen or hydrogen by substances.',\n",
              "       'Oxidation is the gain of oxygen or loss of hydrogen.',\n",
              "       'Reduction is the loss of oxygen or gain of hydrogen.',\n",
              "       'Acid-base indicators are dyes or mixtures of dyes which are used to indicate the presence of acids and bases.',\n",
              "       'When an acid reacts with a metal hydrogen gas is evolved and a corresponding salt is formed.',\n",
              "       'When a base reacts with a metal along with the evolution of hydrogen gas a salt is formed which has a negative ion composed of the metal and oxygen.',\n",
              "       'Acidic and basic solutions in water conduct electricity because they produce hydrogen and hydroxide ions respectively.',\n",
              "       'Mixing concentrated acids or bases with water is a highly exothermic process.',\n",
              "       'Acids and bases neutralise each other to form corresponding salts and water.',\n",
              "       'Water of crystallisation is the fixed number of water molecules present in one formula unit of a salt.',\n",
              "       'Salts have various uses in everyday life and in industries.',\n",
              "       'Elements can be classified as metals and non-metals.',\n",
              "       'Metals are lustrous malleable ductile and are good conductors of heat and electricity. They are solids at room temperature, except mercury which is a liquid.',\n",
              "       'Metals can form positive ions by losing electrons to non-metals.',\n",
              "       'Different metals have different reactivities with water and dilute acids.',\n",
              "       'A list of common metals arranged in order of their decreasing reactivity is known as an activity series.',\n",
              "       'Metals above hydrogen in the Activity series can displace hydrogen from dilute acids.',\n",
              "       'A more reactive metal displaces a less reactive metal from its salt solution.',\n",
              "       'Metals occur in nature as free elements or in the form of their compounds.',\n",
              "       'The extraction of metals from their ores and then refining them for use is known as metallurgy.',\n",
              "       'An alloy is a homogeneous mixture of two or more metals or a metal and a non-metal.',\n",
              "       'The surface of some metals such as iron is corroded when they are exposed to moist air for a long period of time. This phenomenon is known as corrosion.',\n",
              "       'Non-metals form negatively charged ions by gaining electrons when reacting with metals.',\n",
              "       'Non-metals form oxides which are either acidic or neutral.',\n",
              "       'Non-metals do not displace hydrogen from dilute acids. They react with hydrogen to form hydrides.',\n",
              "       'Carbon is a versatile element that forms the basis for all living organisms and many of the things we use.',\n",
              "       'This large variety of compounds is formed by carbon because of its tetravalency and the property of catenation that it exhibits.',\n",
              "       'Covalent bonds are formed by the sharing of electrons between two atoms so that both can achieve a completely filled outermost shell.',\n",
              "       'The ability of carbon to form chains gives rise to a homologous series of compounds in which the same functional group is attached to carbon chains of different lengths.',\n",
              "       'The functional groups such as alcohols aldehydes ketones and carboxylic acids bestow characteristic properties to the carbon compounds that contain them.',\n",
              "       'Carbon and its compounds are some of our major sources of fuels.',\n",
              "       'Ethanol and ethanoic acid are carbon compounds of importance in our daily lives.',\n",
              "       'The action of soaps and detergents is based on the presence of both hydrophobic and hydrophilic groups in the molecule and this helps to emulsify the oily dirt and hence its removal.',\n",
              "       'Elements are classified on the basis of similarities in their properties.',\n",
              "       'Mendeléev arranged the elements in increasing order of their atomic masses and according to their chemical properties.',\n",
              "       'Elements thus arranged show periodicity of properties including atomic size, valency or combining capacity and metallic and non-metallic character.',\n",
              "       'Elements in the Modern Periodic Table are arranged in 18 vertical columns called groups and 7 horizontal rows called periods.',\n",
              "       'Movement of various types can be taken as an indication of life.',\n",
              "       'Maintenance of life requires processes like nutrition respiration transport of materials within the body and excretion of waste products.',\n",
              "       'Autotrophic nutrition involves the intake of simple inorganic materials from the environment and using an external energy source like the Sun to synthesise complex high-energy organic material',\n",
              "       'Heterotrophic nutrition involves the intake of complex material prepared by other organisms.',\n",
              "       'In human beings the food eaten is broken down by various steps along the alimentary canal and the digested food is absorbed in the small intestine to be sent  to all cells in the body.',\n",
              "       'During the process of respiration organic compounds such as glucose are broken down to provide energy in the form of ATP. ATP is used to provide energy for other  reactions in the cell.',\n",
              "       'Respiration may be aerobic or anaerobic. Aerobic respiration makes more energy available to the organism.',\n",
              "       'In human beings the transport of materials such as oxygen carbon dioxide foodand excretory products is a function of the circulatory system. The circulatory system consists of the heart, blood and blood vessels.',\n",
              "       'In highly differentiated plants transport of water minerals food and other materials is a function of the vascular tissue which consists of xylem and phloem.',\n",
              "       'In human beings excretory products in the form of soluble nitrogen compounds are removed by the nephrons in the kidneys.',\n",
              "       'Plants use a variety of techniques to get rid of waste material. For example waste material may be stored in the cell-vacuoles or as gum and resin removed in the falling leaves or excreted into the surrounding soil.',\n",
              "       'Control and coordination are the functions of the nervous system and hormones in our bodies.',\n",
              "       'The responses of the nervous system can be classified as reflex action voluntary action or involuntary action.',\n",
              "       'The nervous system uses electrical impulses to transmit messages.',\n",
              "       'The nervous system gets information from our sense organs and acts through our muscles.',\n",
              "       'Chemical coordination is seen in both plants and animals.',\n",
              "       'Hormones produced in one part of an organism move to another part to achieve the desired effect.',\n",
              "       'A feedback mechanism regulates the action of the hormones.',\n",
              "       'Reproduction unlike other life processes is not essential to maintain the life of an individual organism.',\n",
              "       'Reproduction involves creation of a DNA copy and additional cellular apparatus by the cell involved in the process.',\n",
              "       'Various organisms use different modes of reproduction depending on their body design.',\n",
              "       'In fission many bacteria and protozoa simply divide into two or more daughter cells.',\n",
              "       'Organisms such as hydra can regenerate if they are broken into pieces. They can also give out buds which mature into new individuals.',\n",
              "       'Roots stems and leaves of some plants develop into new plants through vegetative propagation',\n",
              "       'These are examples of asexual reproduction where new generations are created from a single individual.',\n",
              "       'Sexual reproduction involves two individuals for the creation of a new individual.',\n",
              "       'DNA copying mechanisms creates variations which are useful for ensuring the survival of the species. Modes of sexual reproduction allow for greater variation to be generated.',\n",
              "       'Reproduction in flowering plants involves transfer of pollen grains from the anther to the stigma which is referred to as pollination. This is followed by fertilisation.',\n",
              "       'Changes in the body at puberty such as increase in breast size in girls and new facial hair growth in boys are signs of sexual maturation.',\n",
              "       'The male reproductive system in human beings consists of testes which produce sperms vas deferens seminal vesicles prostate gland urethra and penis.',\n",
              "       'The female reproductive system in human beings consists of ovaries fallopian tubes uterus and vagina.',\n",
              "       'Sexual reproduction in human beings involves the introduction of sperm in the vagina of the female. Fertilisation occurs in the fallopian tube.',\n",
              "       'Speciation may take place when variation is combined with geographical isolation.',\n",
              "       'Evolutionary relationships are traced in the classification of organisms.',\n",
              "       'Tracing common ancestors back in time leads us to the idea that at some point of time non-living material must have given rise to life.',\n",
              "       'Evolution can be worked out by the study of not just living species but also fossils.',\n",
              "       'Complex organs may have evolved because of the survival advantage of even the intermediate stages.',\n",
              "       'Organs or features may be adapted to new functions during the course of evolution.',\n",
              "       'Light seems to travel in straight lines.',\n",
              "       'Mirrors and lenses form images of objects. Images can be either real or virtual, depending on the position of the object.',\n",
              "       'The reflecting surfaces of all types obey the laws of reflection. The refracting surfaces obey the laws of refraction.',\n",
              "       'Mirror formula gives the relationship between the object-distance (u) image-distance (v) and focal length (f) of a spherical mirror.',\n",
              "       'The focal length of a spherical mirror is equal to half its radius of curvature.',\n",
              "       'The magnification produced by a spherical mirror is the ratio of the height of the image to the height of the object.',\n",
              "       'A light ray travelling obliquely from a denser medium to a rarer medium bends away from the normal.',\n",
              "       'A light ray bends towards the normal when it travels obliquely from a rarer to a denser medium.',\n",
              "       'The speed of light is different in different media.',\n",
              "       'The refractive index of a transparent medium is the ratio of the speed of light in vacuum to that in the medium.',\n",
              "       'Power of a lens is the reciprocal of its focal length.',\n",
              "       'Lens formula gives the relationship between the object-distance (u) image-distance (v) and the focal length (f) of a spherical lens.',\n",
              "       'The ability of the eye to focus on both near and distant objects by adjusting its focal length is called the accommodation of the eye.',\n",
              "       'The smallest distance at which the eye can see objects clearly without strain is called the near point of the eye or the least distance of distinct vision. For a young adult with normal vision it is about 25 cm.',\n",
              "       'The common refractive defects of vision include myopia hypermetropia and presbyopia.',\n",
              "       'Myopia (short-sightedness – the image of distant objects is focussed before the retina) is corrected by using a concave lens of suitable power.',\n",
              "       'Hypermetropia (far-sightedness – the image of nearby objects is focussed beyond the retina) is corrected by using a convex lens of suitable power.',\n",
              "       'The splitting of white light into its component colours is called dispersion.',\n",
              "       'Scattering of light causes the blue colour of sky and the reddening of the Sun at sunrise and sunset.',\n",
              "       'A stream of electrons moving through a conductor constitutes an electric current. Conventionally, the direction of current is taken opposite to the direction of flow of electrons.',\n",
              "       'The SI unit of electric current is ampere.',\n",
              "       'To set the electrons in motion in an electric circuit we use a cell or a battery. A cell generates a potential difference across its terminals. It is measured in volts (V).',\n",
              "       'Resistance is a property that resists the flow of electrons in a conductor.',\n",
              "       'The potential difference across the ends of a resistor is directly  proportional to the current through it, provided its temperature remains the same.',\n",
              "       'The resistance of a conductor depends directly on its length inversely on its area of cross-section and also on the material of the conductor.',\n",
              "       'The equivalent resistance of several resistors in series is equal to the sum of their individual resistances.',\n",
              "       'A magnetic field exists in the region surrounding a magnet in which the force of the magnet can be detected.',\n",
              "       'Field lines are used to represent a magnetic field.',\n",
              "       'A metallic wire carrying an electric current has associated with it a magnetic field.',\n",
              "       'The field lines about the wire consist of a series of concentric circles whose direction is given by the right-hand rule.',\n",
              "       'The pattern of the magnetic field around a conductor due to an electric current flowing through it depends on the shape of the conductor.',\n",
              "       'The magnetic field of a solenoid carrying a current is similar to that of a bar magnet.',\n",
              "       'An electromagnet consists of a core of soft iron wrapped around with a coil of insulated copper wire.',\n",
              "       'A current-carrying conductor when placed in a magnetic field experiences a force.',\n",
              "       'If the direction of the field and that of the current are mutually perpendicular to each other, then the force acting on the conductor will be perpendicular to both and will be given by Fleming’s left-hand rule',\n",
              "       'The phenomenon of electromagnetic induction is the production of induced current in a coil placed in a region where the magnetic field changes with time.',\n",
              "       'A generator converts mechanical energy into electrical energy. It works on the basis of electromagnetic induction.',\n",
              "       'Fuse is the most important safety device used for protecting the circuits due to short-circuiting or overloading of the circuits.',\n",
              "       'Many of the sources ultimately derive their energy from the Sun.',\n",
              "       'The producers make the energy from sunlight available to the rest of the ecosystem.',\n",
              "       'The various components of an ecosystem are interdependent.',\n",
              "       'There is a loss of energy as we go from one trophic level to the next, this limits the number of trophic levels in a food-chain.',\n",
              "       'The use of chemicals like CFCs has endangered the ozone layer. Since the ozone layer protects against the ultraviolet radiation from the Sun, this could damage the environment.',\n",
              "       'The waste we generate may be biodegradable or non-biodegradable.',\n",
              "       'The disposal of the waste we generate is causing serious environmental problems.',\n",
              "       'Our resources like forests wildlife water coal and petroleum need to be used in a sustainable manner.',\n",
              "       'Matter is made up of small particles.',\n",
              "       'The matter around us exists in three states— solid liquid and gas.',\n",
              "       'The forces of attraction between the particles are maximum in solids, intermediate in liquids and minimum in gases.',\n",
              "       'The spaces in between the constituent particles and kinetic energy of the particles are minimum in the case of solids, intermediate in liquids and maximum in gases.',\n",
              "       'The states of matter are inter-convertible. The state of matter can be changed by changing temperature or pressure.',\n",
              "       'Sublimation is the change of solid state directly to gaseous state without going through liquid state.',\n",
              "       'Deposition is the change of gaseous state directly to solid state without going through liquid state.',\n",
              "       'Boiling is a bulk phenomenon. Particles from the bulk (whole) of the liquid change into vapour state.',\n",
              "       'Evaporation is a surface phenomenon. Particles from the surface gain enough energy to overcome the forces of attraction present in the liquid and change into the vapour state.',\n",
              "       'The rate of evaporation depends upon the surface area exposed to the atmosphere the temperature the humidity and the wind speed.',\n",
              "       'Evaporation causes cooling.',\n",
              "       'Latent heat of vaporisation is the heat energy required to change 1 kg of a liquid to gas at atmospheric pressure at its boiling point.',\n",
              "       'Latent heat of fusion is the amount of heat energy required to change 1 kg of solid into liquid at its melting point.',\n",
              "       'A mixture contains more than one substance (element and/ or compound) mixed in any proportion.',\n",
              "       'Mixtures can be separated into pure substances using appropriate separation techniques.',\n",
              "       'A solution is a homogeneous mixture of two or more substances. The major component of a solution is called the solvent and the minor the solute.',\n",
              "       'The concentration of a solution is the amount of solute present per unit volume or per unit mass of the solution.',\n",
              "       'Materials that are insoluble in a solvent and have particles that are visible to naked eyes, form a suspension. A suspension is a heterogeneous mixture.',\n",
              "       'Colloids are heterogeneous mixtures in which the particle size is too small to be seen with the naked eye, but is big enough to scatter light.',\n",
              "       'The particles are called the dispersed phase and the medium in which they are distributed is called the dispersion medium.',\n",
              "       'An element is a form of matter that cannot be broken down by chemical reactions into simpler substances.',\n",
              "       'A compound is a substance composed of two or more different types of elements chemically combined in a fixed proportion.',\n",
              "       'Properties of a compound are different from its constituent elements whereas a mixture shows the properties of its constituting elements or compounds.',\n",
              "       'During a chemical reaction the sum of the masses of the reactants and products remains unchanged. This is known as the Law of Conservation of Mass.',\n",
              "       'According to the Law of Definite Proportions in a pure chemical compound elements are always present in a definite proportion by mass.',\n",
              "       'An atom is the smallest particle of the element that cannot usually exist independently and retain all its chemical properties.',\n",
              "       'A molecule is the smallest particle of an element or a compound capable of independent existence under ordinary conditions. It shows all the properties of the substance.',\n",
              "       'A chemical formula of a compound shows its constituent elements and the number of atoms of each combining element.',\n",
              "       'Clusters of atoms that act as an ion are called polyatomic ions.',\n",
              "       'The chemical formula of a molecular compound is determined by the valency of each element.',\n",
              "       'In ionic compounds the charge on each ion is used to determine the chemical formula of the compound.',\n",
              "       'Mass of 1 mole of a substance is called its molar mass.',\n",
              "       'Rutherford’s model of the atom proposed that a very tiny nucleus is present inside the atom and electrons revolve around this nucleus.',\n",
              "       'Valency is the combining capacity of an atom.',\n",
              "       'The atomic number of an element is the same as the number of protons in the nucleus of its atom.',\n",
              "       'The mass number of an atom is equal to the number of nucleons in its nucleus.',\n",
              "       'Isotopes are atoms of the same element, which have different mass numbers.',\n",
              "       'Elements are defined by the number of protons they possess.',\n",
              "       'Isobars are atoms having the same mass number but different atomic numbers.',\n",
              "       'The fundamental organisational unit of life is the cell.',\n",
              "       'Cells are enclosed by a plasma membrane composed of lipids and proteins.',\n",
              "       'The cell membrane is an active part of the cell. It regulates the movement of materials between the ordered interior of the cell and the outer environment.',\n",
              "       'In plant cells a cell wall composed mainly of cellulose is located outside the cell membrane.',\n",
              "       'The presence of the cell wall enables the cells of plants fungi and bacteria to exist in hypotonic media without bursting.',\n",
              "       'The nucleus in eukaryotes is separated from the cytoplasm by double-layered membrane and it directs the life processes of the cell.',\n",
              "       'The ER functions both as a passageway for intracellular transport and as a manufacturing surface. Chromoplasts that contain chlorophyll are called chloroplasts and they perform photosynthesis.',\n",
              "       'The primary function of leucoplasts is storage.',\n",
              "       'Most mature plant cells have a large central vacuole that helps to maintain the turgidity of the cell and stores important substances including wastes.',\n",
              "       'Prokaryotic cells have no membrane-bound organelles their chromosomes are composed of only nucleic acid and they have only very small ribosomes as organelles.',\n",
              "       'Cells in organisms divide for growth of body for repalcing dead cells and for forming gametes for reproduction.',\n",
              "       'Tissue is a group of cells similar in structure and function.',\n",
              "       'Plant tissues are of two main types – meristematic and permanent.',\n",
              "       'Meristematic tissue is the dividing tissue present in the growing regions of the plant.',\n",
              "       'Permanent tissues are derived from meristematic tissue once they lose the ability to divide. They are classified as simple and complex tissues.',\n",
              "       'Animal tissues can be epithelial connective muscular and nervous tissue.',\n",
              "       'Depending on shape and function epithelial tissue is classified as squamous cuboidal columnar ciliated and glandular.',\n",
              "       'The different types of connective tissues in our body include areolar tissue adipose tissue bone tendon ligament cartilage and blood.',\n",
              "       'Striated, unstriated and cardiac are three types of muscle tissues.',\n",
              "       'Nervous tissue is made of neurons that receive and conduct impulses.',\n",
              "       'Classification helps us in exploring the diversity of life forms.',\n",
              "       'The classification of life forms is related to their evolution.',\n",
              "       'Plantae and Animalia are further divided into subdivisions on the basis of increasing complexity of body organisation.',\n",
              "       'Plants are divided into five groups: Thallophytes Bryophytes Pteridophytes Gymnosperms and Angiosperms.',\n",
              "       'The binomial nomenclature makes for a uniform way of identification of the vast diversity of life around us.',\n",
              "       'The binomial nomenclature is made up of two words – a generic name and a specific name.',\n",
              "       'Motion is a change of position; it can be described in terms of the distance moved or the displacement.',\n",
              "       'The motion of an object could be uniform or non-uniform depending on whether its velocity is constant or changing.',\n",
              "       'The speed of an object is the distance covered per unit time, and velocity is the displacement per unit time.',\n",
              "       'The acceleration of an object is the change in velocity per unit time.',\n",
              "       'If an object moves in a circular path with uniform speed, its motion is called uniform circular motion.',\n",
              "       'The natural tendency of objects to resist a change in their state of rest or of uniform motion is called inertia.',\n",
              "       'The mass of an object is a measure of its inertia. Its SI unit is kilogram (kg).',\n",
              "       'Force of friction always opposes motion of objects.',\n",
              "       'The rate of change of momentum of an object is proportional to the applied unbalanced force in the direction of the force.',\n",
              "       'The momentum of an object is the product of its mass and velocity and has the same direction as that of the velocity.',\n",
              "       'To every action, there is an equal and opposite reaction and they act on two different bodies.',\n",
              "       'In an isolated system (where there is no external force), the total momentum remains conserved.',\n",
              "       'An object continues to be in a state of rest or of uniform motion along a straight line unless acted upon by an unbalanced force.',\n",
              "       'The law of gravitation states that the force of attraction between any two objects is proportional to the product of their masses and inversely proportional to the square of the distance between them.',\n",
              "       'Gravitation is a weak force unless large masses are involved.',\n",
              "       'The force of gravity decreases with altitude. It also varies on the surface of the earth, decreasing from poles to the equator.',\n",
              "       'The weight of a body is the force with which the earth attracts it.',\n",
              "       'The weight is equal to the product of mass and acceleration due to gravity.',\n",
              "       'The weight may vary from place to place but the mass stays constant.',\n",
              "       'All objects experience a force of buoyancy when they are immersed in a fluid.',\n",
              "       'Objects having density less than that of the liquid in which they are immersed, float on the surface of the liquid. If the density of the object is more than the density of the liquid in which it is immersed then it sinks in the liquid.',\n",
              "       'Work done on an object is defined as the magnitude of the force multiplied by the distance moved by the object in the direction of the applied force.',\n",
              "       'Work done on an object by a force would be zero if the displacement of the object is zero.',\n",
              "       'An object having capability to do work is said to possess energy.',\n",
              "       'An object in motion possesses what is known as the kinetic energy of the object.',\n",
              "       'The energy possessed by a body due to its change in position or shape is called the potential energy.',\n",
              "       'According to the law of conservation of energy energy can only be transformed from one form to another; it can neither be created nor destroyed.',\n",
              "       'Sound is produced due to vibration of different objects.',\n",
              "       'Sound travels as a longitudinal wave through a material medium.',\n",
              "       'Sound travels as successive compressions and rarefactions in the medium.',\n",
              "       'In sound propagation it is the energy of the sound that travels and not the particles of the medium.',\n",
              "       'Sound cannot travel in vacuum.',\n",
              "       'The change in density from one maximum value to the minimum value and again to the maximum value makes one complete oscillation.',\n",
              "       'The distance between two consecutive compressions or two consecutive rarefactions is called the wavelength.',\n",
              "       'The time taken by the wave for one complete oscillation of the density or pressure of the medium is called the time period.',\n",
              "       'The number of complete oscillations per unit time is called the frequency.',\n",
              "       'The speed of sound depends primarily on the nature and the temperature of the transmitting medium.',\n",
              "       'The persistence of sound in an auditorium is the result of repeated reflections of sound and is called reverberation.',\n",
              "       'Sound properties such as pitch loudness and quality are determined by the corresponding wave properties.',\n",
              "       'Sound waves with frequencies below the audible range are termed “infrasonic” and those above the audible range are termed “ultrasonic”.',\n",
              "       'Health is a state of physical mental and social well-being.',\n",
              "       'Diseases are classified as acute or chronic depending on their duration.',\n",
              "       'Infectious agents belong to different categories of organisms and may be unicellular and microscopic or multicellular.',\n",
              "       'The category to which a disease-causing organism belongs decides the type of treatment.',\n",
              "       'Infectious agents are spread through air water physical contact or vectors.',\n",
              "       'Prevention of disease is more desirable than its successful treatment.',\n",
              "       'Infectious diseases can be prevented by public health hygiene measures that reduce exposure to infectious agents.',\n",
              "       'Infectious diseases can also be prevented by using immunisation.',\n",
              "       'Effective prevention of infectious diseases in the community requires that everyone should have access to public hygiene and immunisation.',\n",
              "       'Life on Earth depends on resources like soil water and air and energy from the Sun.',\n",
              "       'Uneven heating of air over land and water-bodies causes winds.',\n",
              "       'Evaporation of water from water -bodies and subsequent condensation give us rain.',\n",
              "       'Rainfall patterns depend on the prevailing wind patterns in an area.',\n",
              "       'Various nutrients are used again and again in a cyclic fashion. This leads to a certain balance between the various components of the biosphere.',\n",
              "       'Pollution of air water and soil affect the quality of life and harm the biodiversity.',\n",
              "       'We need to conserve our natural resources and use them in a sustainable manner.',\n",
              "       'Manure and fertilizers are the main sources of nutrient supply to crops.',\n",
              "       'Organic farming is a farming system with minimal or no use of chemicals as fertilizers herbicides pesticides etc. and with a maximum input of organic manures recyled farm wastes  and bio-agents, with healthy cropping systems.',\n",
              "       'Mixed farming is a system of farming on a particular farm which includes crop production, raising of livestock etc.',\n",
              "       'Mixed cropping is growing of two or more crops simultaneously on the same piece of land.',\n",
              "       'Growing two or more crops in definite row patterns is known as inter-cropping.',\n",
              "       'The growing of different crops on a piece of land in pre-planned succession is called crop rotation.',\n",
              "       'Varietal improvement is required for higher yield good quality biotic and abiotic resistance shortening the maturity duration wider adaptability and desirable agronomic characteristics.',\n",
              "       'Farm animals require proper care and management such as shelter feeding breeding and disease control. This is called animal husbandry.',\n",
              "       'Poultry farming is done to raise domestic fowls. Poultry production includes egg production and broiler production for poultry meat.',\n",
              "       'To enhance poultry production cross breeding is done between Indian and exotic breeds for variety improvement.',\n",
              "       'Marine fish capture is done by fishing nets guided by echo- sounders and satellites.',\n",
              "       'Composite fish culture system is commonly used for fish farming.',\n",
              "       'In order to provide food to our growing population we need to adopt certain agricultural practices.',\n",
              "       'Same kind of plants cultivated at a place constitute a crop.',\n",
              "       'In India crops can be broadly categorised into two types based on seasons - rabi and kharif crops.',\n",
              "       'It is necessary to prepare soil by tilling and levelling. Ploughs and levellers are used for this purpose.',\n",
              "       'Sowing of seeds at appropriate depths and distances gives good yield. Good variety of seeds are sown after selection of healthy seeds. Sowing is done by seed drills.',\n",
              "       'Soil needs replenishment and enrichment through the use of organic manure and fertilisers. Use of chemical fertilisers has increased tremendously with the introduction of new crop varieties.',\n",
              "       'Supply of water to crops at appropriate intervals is called irrigation.',\n",
              "       'Weeding involves removal of unwanted and uncultivated plants called weeds.',\n",
              "       'Harvesting is the cutting of the mature crop manually or by machines.',\n",
              "       'Separation of the grains from the chaff is called threshing.',\n",
              "       'Proper storage of grains is necessary to protect them from pests and microorganisms.',\n",
              "       'Food is also obtained from animals for which animals are reared. This is called animal husbandry.',\n",
              "       'Microorganisms are too small and are not visible to the unaided eye',\n",
              "       'They can live in all kinds of environment ranging from ice cold climate to hot springs and deserts to marshy lands.',\n",
              "       'Microorganisms are found in air water and in the bodies of plants and animals.',\n",
              "       'Microorganisms may be unicellular or multicellular.',\n",
              "       'Microorganisms include bacteria fungi protozoa and some algae. Viruses though different from the above mentioned living organisms are considered microbes.',\n",
              "       'Viruses are quite different from other microorganisms. They reproduce only inside the host organism: bacterium, plant or animal cell.',\n",
              "       'Some microorganisms are useful for commercial production of medicines and alcohol.',\n",
              "       'Some microorganisms decompose the organic waste and dead plants and animals into simple substances and clean up the environment.',\n",
              "       'Protozoans cause serious diseases like dysentery and malaria.',\n",
              "       'Some of the microorganisms grow on our food and cause food poisoning.',\n",
              "       'Some microorganisms reside in the root nodules of leguminous plants. They can fix nitrogen from air into soil and increase the soil fertility.',\n",
              "       'Some bacteria present in the soil fix nitrogen from the atmosphere and convert into nitrogenous compounds.',\n",
              "       'Certain bacteria convert compounds of nitrogen present in the soil into nitrogen gas which is released to the atmosphere.',\n",
              "       'Synthetic fibres and plastics like natural fibres are made of very large units called polymers. Polymers are made up of many smaller units',\n",
              "       'While natural fibres are obtained from plants and animals synthetic fibres are obtained by chemical processing of petrochemicals. Like natural fibres these fibres can also be woven into fabrics.',\n",
              "       'Synthetic fibres find uses ranging from many household articles like ropes buckets  furniture  containers  etc.  to highly specialised uses in aircrafts ships spacecrafts healthcare etc.',\n",
              "       'Depending upon the types of chemicals used for manufacturing synthetic fibres they are called Rayon Nylon Polyester and Acrylic.',\n",
              "       'The different types of fibres differ from one another in their strength water absorbing capacity nature of burning cost durability etc.',\n",
              "       'Today life without plastics cannot be imagined. Be it home or outside plastic is everywhere.',\n",
              "       'The waste created by plastics is not environment friendly. On burning plastics release poisonous gases. On dumping in the ground they may take years to degenerate. This is because of their non-biodegradable nature.',\n",
              "       'Metals are lustrous whereas non-metals have no lustre.',\n",
              "       'Generally metals are malleable and ductile. Non-metals do not have these properties.',\n",
              "       'Generally metals are good conductors of heat and electricity but non-metals are poor conductors.',\n",
              "       'On burning metals react with oxygen to produce metal oxides which are basic in nature. Non-metals react with oxygen to produce non- metallic oxides which are acidic in nature.',\n",
              "       'Some metals react with water to produce metal hydroxides and hydrogen gas. Generally non-metals do not react with water.',\n",
              "       'Metals react with acids and produce metal salts and hydrogen gas. Generally non-metals do not react with acids.',\n",
              "       'Some metals react with bases to produce hydrogen gas.',\n",
              "       'More reactive metals displace less reactive metals from their compounds in aqueous solutions.',\n",
              "       'Metals and non-metals are used widely in every day life.',\n",
              "       'Coal petroleum and natural gas are fossil fuels.',\n",
              "       'Fossil fuels were formed from the dead remains of living organisms millions of years ago.',\n",
              "       'Fossil fuels are exhaustible resources.',\n",
              "       'Coke coal tar and coal gas are the products of coal.',\n",
              "       'Petroleum gas petrol diesel kerosene paraffin wax lubricating oil are obtained by refining petroleum',\n",
              "       'Coal and petroleum resources are limited. We should use them judiciously.',\n",
              "       'The substances which burn in air are called combustible.',\n",
              "       'Oxygen (in air) is essential for combustion.',\n",
              "       'Ignition temperature is the lowest temperature at which a combustible substance catches fire.',\n",
              "       'During the process of combustion, heat and light are given out.',\n",
              "       'Inflammable substances have very low ignition temperature.',\n",
              "       'There are various types of combustions such as rapid combustion spontaneous combustion explosion etc.',\n",
              "       'There are three different zones of a flame - dark zone luminous zone and non-luminous zone.',\n",
              "       'An ideal fuel is cheap readily available readily combustible and easy to transport. It has high calorific value. It does not produce gases or residues that pollute the environment.',\n",
              "       'Fuels differ in their efficiency and cost.',\n",
              "       'Unburnt carbon particles in air are dangerous pollutants causing respiratory problems.',\n",
              "       'Incomplete combustion of a fuel gives poisonous carbon monoxide gas.',\n",
              "       'Biodiversity refers to the variety of living organisms in a specific area.',\n",
              "       'Plants and animals of a particular area are known as the flora and fauna of that area.',\n",
              "       'Endemic species are found only in a particular area.',\n",
              "       'Endangered species are those which are facing the danger of extinction.',\n",
              "       'Reforestation is the restocking of destroyed    forests by planting new trees.',\n",
              "       'Some organisms are single-celled while others contain large number of cells.',\n",
              "       'The single cell of unicellular organisms performs all the basic functions performed by a variety of cells in multicellular organisms.',\n",
              "       'The cell has three main parts: (i) the cell membrane (ii) cytoplasm which contains smaller components called organelles and (iii) the nucleus.',\n",
              "       'Nucleus is separated from cytoplasm by a nuclear membrane.',\n",
              "       'Nucleus is separated from cytoplasm by a nuclear membrane.',\n",
              "       'Plant cells differ from animal cells in having an additional layer around the cell membrane termed cell wall.',\n",
              "       'Coloured bodies called plastids are found in the plant cells only. Green plastids containing chlorophyll are called chloroplasts.',\n",
              "       'Plant cell has a big central vacuole unlike a number of small vacuoles in animal cells.',\n",
              "       'Reproduction resulting from the fusion of male and female gametes is called sexual reproduction.',\n",
              "       'The reproductive organs in the female include ovaries oviducts and uterus.',\n",
              "       'The reproductive organs in male include testes sperm ducts and penis.',\n",
              "       'The ovary produces female gametes called ova and the testes produce male gametes called sperms.',\n",
              "       'The fusion of ovum and sperm is called fertilisation. The fertilised egg is called a zygote.',\n",
              "       'Fertilisation that takes place inside the female body is called internal fertilisation. This is observed in human beings and other animals such as hens, cows and dogs.',\n",
              "       'The zygote divides repeatedly to give rise to an embryo.',\n",
              "       'Fertilisation that takes place outside the female body is called external fertilisation. This is observed in frogs fish starfish etc.',\n",
              "       'The embryo gets embedded in the wall of the uterus for further development.',\n",
              "       'The stage of the embryo in which all the body parts are identifiable is called foetus.',\n",
              "       'Animals such as hen frog lizard and butterfly which lay eggs are called oviparous animals.',\n",
              "       'The transformation of the larva into adult through drastic changes is called metamorphosis.',\n",
              "       'The type of reproduction in which only a single parent is involved is called asexual reproduction.',\n",
              "       'In hydra new individuals develop from buds. This method of asexual reproduction is called budding.',\n",
              "       'Amoeba reproduces by dividing itself into two. This type of asexual reproduction is called binary fission.',\n",
              "       'Force could be a push or a pull.',\n",
              "       'A force arises due to the interaction between two objects.',\n",
              "       'Force has magnitude as well as direction.',\n",
              "       'A change in the speed of an object or the direction of its motion or both implies a change in its state of motion.',\n",
              "       'Force acting on an object may cause a change in its state of motion or a change in its shape.',\n",
              "       'A force can act on an object with or without being in contact with it.',\n",
              "       'Force per unit area is called pressure.',\n",
              "       'Liquids and gases exert pressure on the walls of their containers.',\n",
              "       'The pressure exerted by air around us is known as atmospheric pressure.',\n",
              "       'Friction opposes the relative motion between two surfaces in contact. It acts on both the surfaces.',\n",
              "       'Friction depends on the nature of surfaces in contact.',\n",
              "       'For a given pair of surfaces friction depends upon the state of smoothness of those surfaces.',\n",
              "       'Friction depends on how hard the two surfaces press together.',\n",
              "       'Static friction comes into play when we try to move an object at rest.',\n",
              "       'Sliding friction comes into play when an object is sliding over another.',\n",
              "       'Sliding friction is smaller than static friction.',\n",
              "       'Friction is important for many of our activities.',\n",
              "       'Friction can be increased by making a surface rough.',\n",
              "       'The sole of the shoes and the tyres of the vehicle are treaded to increase friction.',\n",
              "       'When one body rolls over another body, rolling friction comes into play. Rolling friction is smaller than sliding friction.',\n",
              "       'In many machines friction is reduced by using ball bearings.',\n",
              "       'Fluid friction can be minimised by giving suitable shapes to bodies moving in fluids.',\n",
              "       'Sound is produced by vibrating objects.',\n",
              "       'In human beings the vibration of the vocal cords produces sound.',\n",
              "       'Sound travels through a medium (gas, liquid or solid). It cannot travel in vacuum.',\n",
              "       'The eardrum senses the vibrations of sound, It sends the signals to the brain. This process is called hearing.',\n",
              "       'The number of oscillations or vibrations per second is called the frequency of oscillation.',\n",
              "       'Larger the amplitude of vibration, the louder is the sound.',\n",
              "       'Higher the frequency of vibration the higher is the pitch and shriller is the sound.',\n",
              "       'Unpleasant sounds are called noise.',\n",
              "       'Attempts should be made to minimise noise pollution.',\n",
              "       'Some liquids are good conductors of electricity and some are poor conductors.',\n",
              "       'Most liquids that conduct electricity are solutions of acids, bases and salts.',\n",
              "       'The passage of an electric current through a conducting liquid causes chemical reactions. The resulting effects are called chemical effects of currents.',\n",
              "       'The process of depositing a layer of any desired metal on another material by means of electricity, is called electroplating.',\n",
              "       'There are two kinds of charges — positive charge and negative charge',\n",
              "       'Like charges repel and unlike charges attract each other.',\n",
              "       'The electrical charges produced by rubbing are called static charges.',\n",
              "       'When charges move they constitute an electric current.',\n",
              "       'An electroscope may be used to detect whether a body is charged or not.',\n",
              "       'The process of transfer of charge from a charged object to the earth is called earthing.',\n",
              "       'The process of electric discharge between clouds and the earth or between differentclouds causes lightning.',\n",
              "       'Lightning conductors can protect buildings from the effects of lightning.',\n",
              "       'Light is reflected from all surfaces.',\n",
              "       'Diffused or irregular reflection takes place from rough surfaces.',\n",
              "       'Regular reflection takes place when light is  incident on smooth, polished and regular surfaces.',\n",
              "       'The angle of incidence is equal to the angle of reflection.',\n",
              "       'Incident ray reflected ray and the normal drawn at the point of incidence to the reflecting surface, lie in the same plane.',\n",
              "       'Image formed in a plane mirror undergoes lateral inversion.',\n",
              "       'Two mirrors inclined to each other give multiple images.',\n",
              "       'Splitting of light into its constituent colours is known as dispersion.',\n",
              "       'The phases of the moon occur because we can see only that part of the moon which reflects the light of the Sun towards us.',\n",
              "       'Stars are celestial bodies that emit light of their own. Our sun is also a star.',\n",
              "       'It is convenient to express distances of stars in light years.',\n",
              "       'Stars appear to move from east to west.',\n",
              "       'The pole star appears to be stationary from the Earth, because it is situated close to the direction of the axis of rotation of the Earth.',\n",
              "       'Constellations are groups of stars that appear to form recognisable shapes.',\n",
              "       'The solar system consists of eight planets and a host of asteroids, comets and meteors.',\n",
              "       'A body revolving around another body is called a satellite.',\n",
              "       'Moon is the natural satellite of the Earth. Some planets also have natural satellites.',\n",
              "       'The artificial satellites revolve around the Earth. They are much closer than the moon.',\n",
              "       'Artificial satellites are used for weather forecasting, long distance communication and remote sensing.',\n",
              "       'Air pollution is the contamination of air by  impurities which may have a harmful impact  on the living organisms and the non-living components.',\n",
              "       'Pollutants are the substances which contaminate air and water.',\n",
              "       'Carbon monoxide nitrogen oxides carbon dioxide methane and sulphur dioxide are the major pollutants of air',\n",
              "       'Increasing levels of greenhouse gases like CO 2 are leading to global warming.',\n",
              "       'Water pollution is the contamination of water by substances harmful to life.',\n",
              "       'Water which is purified and fit for drinking is known as potable water'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRZ54gFokNh9"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohj1x7frQJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af4deb7-f900-48b0-d8eb-906ff2ab5a03"
      },
      "source": [
        "len(list(set(labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVWhJ4o0YxZ1"
      },
      "source": [
        "len(list(set(train_data[\"QCLabel\"].values)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oww-EQlFbQWL"
      },
      "source": [
        "def get_cleaned_taxonomy_lo(taxonomy):\n",
        "  cleaned_taxonomy = []\n",
        "  for value in taxonomy:\n",
        "      value = ' '.join(value.lower().split(\">>\"))\n",
        "      # taxonomy_words = [inflection.singularize(val)  for token in value for val in token.split(\" \") if val.isalpha()]\n",
        "      cleaned_taxonomy.append( value )\n",
        "  return cleaned_taxonomy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXLemgwUbS_r"
      },
      "source": [
        "#when testing for Lo data\n",
        "test_labels = list(set(lo_data[\"taxonomy\"].values))\n",
        "emb_data_test = get_cleaned_taxonomy_lo(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91PVN-_ye8Aq"
      },
      "source": [
        "import pandas as pd\n",
        "targets_1 = pd.read_csv(\"targets_ARC.csv\")\n",
        "targets = targets_1[\"targets\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjinn0gXkNuP"
      },
      "source": [
        "# When testing for ARC data\n",
        "# course_taxonomy\n",
        "test_labels = targets\n",
        "emb_data_test = get_cleaned_taxonomy(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wYoTCt12Wkf"
      },
      "source": [
        "targets = np.array(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp86I3HkhvwB",
        "outputId": "745c92d4-a389-421c-8f4c-a50b1e911870"
      },
      "source": [
        "len(emb_data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0gGbZbEgS_y"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sent_model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wl0RJ3SSW4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c93dca-62db-4b3d-ec71-227b49d88f72"
      },
      "source": [
        "# taxonomy_vectors = []\"\"\n",
        "taxonomy_vectors = sent_model.encode(emb_data_test)\n",
        "taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
        "taxonomy_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nso39n1N_po_"
      },
      "source": [
        "model = MulticlassClassifier('bert-base-uncased')\n",
        "model.load_state_dict(torch.load('model_euclidean_SENT_BERT_cos_attention_2_V3/model_weights'))\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe4qYkV2C4fX"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in test_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNdlve8AJcCO"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBN7kS5ebZ1"
      },
      "source": [
        "len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe0otXOPg7z0"
      },
      "source": [
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j0Q68gjYl8V"
      },
      "source": [
        "test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-1RvWKfV7C"
      },
      "source": [
        "test_poincare_tensor.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPCktQT9DVT4"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,1,largest=True)\n",
        "  predictions.append(targets[indices.cpu().numpy()])\n",
        "print(len(predictions))\n",
        "\n",
        "\n",
        "print('    DONE.')\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_poincare_tensor = test_poincare_tensor"
      ],
      "metadata": {
        "id": "yPro3kvb6EwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "test_labels = np.array(test_labels)\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "unique_poincare_tensor = unique_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "    \n",
        "  distances = cos(outputs,test_poincare_tensor)#torch.topk(cos(outputs,test_poincare_tensor),20,largest=True)\n",
        "  distances,indices = torch.topk(distances,2,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))\n",
        "\n",
        "\n",
        "print('    DONE.')\n",
        "# predictions"
      ],
      "metadata": {
        "id": "kv1MpZz31Oyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wj2gXYFkrQ"
      },
      "source": [
        "# labels=test_data['QCLabel'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjon8k2B2GHc"
      },
      "source": [
        "targets_1 = pd.read_csv(\"targets_ARC.csv\")\n",
        "\n",
        "labels = targets_1[\"targets\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt8hvs-CZfn"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n",
        "LE= LabelEncoder()\n",
        "labels = LE.fit_transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb4gTNgGKUP"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbohQzAhlYRN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azkCfK-bCoXd"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "  final_predictions.append(LE.transform(prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQrlczKxMwzZ"
      },
      "source": [
        "len(final_predictions[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwPWqwSkeVKg"
      },
      "source": [
        "test_labels = LE.transform(test_data[\"QCLabel\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfhsZh6EeYHK",
        "outputId": "9b3de708-ff05-40ba-eea1-795b756f57b4"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=10)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=10)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 10)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 10) (1400,)\n",
            "update_recall:  0.85\n",
            "recall 0.85\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1190.0, 210.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[350, 349, 348, ..., 341, 340, 337],\n",
            "       [285, 283, 282, ...,  46,  45,  44],\n",
            "       [287, 269, 256, ..., 239,  82,  77],\n",
            "       ...,\n",
            "       [331, 329, 304, ..., 189, 188, 146],\n",
            "       [172, 171, 138, ..., 101,  95,  86],\n",
            "       [347,  26,  17, ...,   7,   6,   2]]), indices=array([[3, 0, 2, ..., 8, 9, 4],\n",
            "       [3, 8, 4, ..., 7, 2, 9],\n",
            "       [7, 0, 6, ..., 8, 3, 5],\n",
            "       ...,\n",
            "       [6, 9, 8, ..., 2, 1, 5],\n",
            "       [8, 2, 1, ..., 7, 9, 3],\n",
            "       [9, 4, 6, ..., 1, 2, 5]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is-KTAENfB6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c32f44-fb62-4952-f499-69e649482761"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",s|ess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 20) (1400,)\n",
            "update_recall:  0.905\n",
            "recall 0.905\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1267.0, 133.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[351, 350, 349, ..., 198,  81,  80],\n",
            "       [291, 287, 285, ...,  38,  35,  31],\n",
            "       [287, 272, 271, ...,  82,  78,  77],\n",
            "       ...,\n",
            "       [334, 333, 331, ..., 188, 156, 146],\n",
            "       [182, 181, 180, ..., 101,  95,  86],\n",
            "       [347, 341, 340, ...,   6,   2,   1]]), indices=array([[10,  3,  0, ..., 17, 16, 15],\n",
            "       [12, 10,  3, ..., 16, 14, 19],\n",
            "       [ 7, 18, 19, ...,  3, 11,  5],\n",
            "       ...,\n",
            "       [17, 13,  6, ...,  1, 10,  5],\n",
            "       [17, 10, 14, ...,  7,  9,  3],\n",
            "       [ 9, 14, 15, ...,  2,  5, 12]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI1jhndp6cEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c623d07-6557-410e-c51d-bf58aec7c21b"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 15)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 15)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 15)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 15) (1400,)\n",
            "precision 0.05919047619047619\n",
            "update_recall:  0.8878571428571429\n",
            "recall 0.8878571428571429\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1243.0, 157.0, 1243.0, 19757.0]\n",
            "TMP_RANK:  TopKV2(values=array([[351, 350, 349, ..., 337, 336, 335],\n",
            "       [291, 287, 285, ...,  45,  44,  35],\n",
            "       [287, 269, 266, ...,  82,  78,  77],\n",
            "       ...,\n",
            "       [333, 331, 329, ..., 188, 156, 146],\n",
            "       [181, 180, 172, ..., 101,  95,  86],\n",
            "       [347, 341,  26, ...,   6,   2,   1]]), indices=array([[10,  3,  0, ...,  4, 13, 12],\n",
            "       [12, 10,  3, ...,  2,  9, 14],\n",
            "       [ 7,  0, 14, ...,  3, 11,  5],\n",
            "       ...,\n",
            "       [13,  6,  9, ...,  1, 10,  5],\n",
            "       [10, 14,  8, ...,  7,  9,  3],\n",
            "       [ 9, 14,  4, ...,  2,  5, 12]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaA9z5n3mZz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab40460-ac5d-4c1f-f2fb-c49822c08293"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 10)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 10)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 10)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 10) (1400,)\n",
            "precision 0.085\n",
            "update_recall:  0.85\n",
            "recall 0.85\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1190.0, 210.0, 1190.0, 12810.0]\n",
            "TMP_RANK:  TopKV2(values=array([[350, 349, 348, ..., 341, 340, 337],\n",
            "       [285, 283, 282, ...,  46,  45,  44],\n",
            "       [287, 269, 256, ..., 239,  82,  77],\n",
            "       ...,\n",
            "       [331, 329, 304, ..., 189, 188, 146],\n",
            "       [172, 171, 138, ..., 101,  95,  86],\n",
            "       [347,  26,  17, ...,   7,   6,   2]]), indices=array([[3, 0, 2, ..., 8, 9, 4],\n",
            "       [3, 8, 4, ..., 7, 2, 9],\n",
            "       [7, 0, 6, ..., 8, 3, 5],\n",
            "       ...,\n",
            "       [6, 9, 8, ..., 2, 1, 5],\n",
            "       [8, 2, 1, ..., 7, 9, 3],\n",
            "       [9, 4, 6, ..., 1, 2, 5]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkKaMSEJnJUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d557bf-79f2-4c47-e555-595c241d2ae6"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 5)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 5) (1400,)\n",
            "precision 0.15614285714285714\n",
            "update_recall:  0.7807142857142857\n",
            "recall 0.7807142857142857\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1093.0, 307.0, 1093.0, 5907.0]\n",
            "TMP_RANK:  TopKV2(values=array([[351, 349, 348, 344, 342],\n",
            "       [281,  47,  46,  45,  44],\n",
            "       [269, 268, 254, 251, 239],\n",
            "       ...,\n",
            "       [194, 193, 192, 189, 188],\n",
            "       [180, 138, 137, 127, 104],\n",
            "       [  9,   8,   7,   6,   2]]), indices=array([[4, 0, 1, 2, 3],\n",
            "       [4, 2, 1, 0, 3],\n",
            "       [1, 3, 0, 4, 2],\n",
            "       ...,\n",
            "       [0, 4, 3, 2, 1],\n",
            "       [4, 1, 0, 3, 2],\n",
            "       [0, 3, 2, 1, 4]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 3)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 3)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 3)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz_vHGa7GUi-",
        "outputId": "0ccb3cb0-dffc-4eb0-f88b-b7b912ebd936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 3) (1400,)\n",
            "precision 0.23809523809523808\n",
            "update_recall:  0.7142857142857143\n",
            "recall 0.7142857142857143\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000.0, 400.0, 1000.0, 3200.0]\n",
            "TMP_RANK:  TopKV2(values=array([[349, 348, 344],\n",
            "       [ 47,  46,  45],\n",
            "       [269, 254, 239],\n",
            "       ...,\n",
            "       [194, 189, 188],\n",
            "       [138, 137, 104],\n",
            "       [  9,   7,   6]]), indices=array([[0, 1, 2],\n",
            "       [2, 1, 0],\n",
            "       [1, 0, 2],\n",
            "       ...,\n",
            "       [0, 2, 1],\n",
            "       [1, 0, 2],\n",
            "       [0, 2, 1]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPqYvRNIrRg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d3d6c1-fa69-49a3-b066-d7771c19fd5b"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(test_labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 1)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 1)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 1)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 1) (1400,)\n",
            "precision 0.4957142857142857\n",
            "update_recall:  0.4957142857142857\n",
            "recall 0.4957142857142857\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 694.0, 706.0, 694.0, 706.0]\n",
            "TMP_RANK:  TopKV2(values=array([[349],\n",
            "       [ 45],\n",
            "       [254],\n",
            "       ...,\n",
            "       [194],\n",
            "       [137],\n",
            "       [  9]]), indices=array([[0],\n",
            "       [0],\n",
            "       [0],\n",
            "       ...,\n",
            "       [0],\n",
            "       [0],\n",
            "       [0]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUzf5IbMDfHp"
      },
      "source": [
        "# LO classification o/p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnSjuzMDDhwe"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 1)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 1)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 1)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXaBO4GEDhy_",
        "outputId": "018b3982-5196-4bca-bd36-c5ba89a01827"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 2)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 2)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 2)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(417, 2) (417,)\n",
            "precision 0.23261390887290168\n",
            "update_recall:  0.46522781774580335\n",
            "recall 0.46522781774580335\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 194.0, 223.0, 194.0, 640.0]\n",
            "TMP_RANK:  TopKV2(values=array([[ 5,  4],\n",
            "       [ 5,  3],\n",
            "       [ 5,  4],\n",
            "       [ 5,  4],\n",
            "       [ 5,  4],\n",
            "       [ 5,  4],\n",
            "       [ 5,  4],\n",
            "       [27,  0],\n",
            "       [ 5,  1],\n",
            "       [ 5,  4],\n",
            "       [ 5,  4],\n",
            "       [ 5,  4],\n",
            "       [30,  4],\n",
            "       [ 5,  0],\n",
            "       [27,  0],\n",
            "       [ 4,  0],\n",
            "       [ 5,  0],\n",
            "       [30,  0],\n",
            "       [27,  0],\n",
            "       [27,  0],\n",
            "       [34,  0],\n",
            "       [34, 27],\n",
            "       [41, 29],\n",
            "       [29, 27],\n",
            "       [27,  0],\n",
            "       [27,  0],\n",
            "       [ 5,  4],\n",
            "       [34,  0],\n",
            "       [27,  0],\n",
            "       [29, 27],\n",
            "       [35,  4],\n",
            "       [ 4,  2],\n",
            "       [ 2,  0],\n",
            "       [34,  0],\n",
            "       [22,  2],\n",
            "       [ 4,  2],\n",
            "       [41,  1],\n",
            "       [ 2,  1],\n",
            "       [ 2,  1],\n",
            "       [47,  2],\n",
            "       [20,  2],\n",
            "       [43,  0],\n",
            "       [30,  0],\n",
            "       [34,  0],\n",
            "       [34,  0],\n",
            "       [40,  5],\n",
            "       [21, 13],\n",
            "       [46, 36],\n",
            "       [42, 20],\n",
            "       [43, 20],\n",
            "       [36, 20],\n",
            "       [22,  3],\n",
            "       [39, 38],\n",
            "       [29, 27],\n",
            "       [45,  0],\n",
            "       [35,  0],\n",
            "       [46, 10],\n",
            "       [36, 13],\n",
            "       [37, 12],\n",
            "       [16, 12],\n",
            "       [16, 12],\n",
            "       [30,  1],\n",
            "       [36,  3],\n",
            "       [36, 17],\n",
            "       [36, 18],\n",
            "       [18,  3],\n",
            "       [36,  3],\n",
            "       [18,  3],\n",
            "       [22,  3],\n",
            "       [17, 10],\n",
            "       [36, 18],\n",
            "       [36, 18],\n",
            "       [18, 17],\n",
            "       [17, 10],\n",
            "       [42, 36],\n",
            "       [36, 18],\n",
            "       [36, 18],\n",
            "       [36, 18],\n",
            "       [35, 17],\n",
            "       [34, 17],\n",
            "       [37, 19],\n",
            "       [37, 22],\n",
            "       [36,  3],\n",
            "       [33, 11],\n",
            "       [24, 23],\n",
            "       [23, 19],\n",
            "       [25, 23],\n",
            "       [43, 19],\n",
            "       [25, 19],\n",
            "       [25, 19],\n",
            "       [25, 10],\n",
            "       [25, 23],\n",
            "       [31, 25],\n",
            "       [25, 23],\n",
            "       [23, 19],\n",
            "       [43, 38],\n",
            "       [25, 19],\n",
            "       [43, 25],\n",
            "       [25, 19],\n",
            "       [25, 19],\n",
            "       [25, 19],\n",
            "       [24, 23],\n",
            "       [25, 23],\n",
            "       [26, 12],\n",
            "       [16, 12],\n",
            "       [26, 12],\n",
            "       [26, 12],\n",
            "       [26, 12],\n",
            "       [34, 12],\n",
            "       [26, 12],\n",
            "       [26, 16],\n",
            "       [26, 16],\n",
            "       [43, 26],\n",
            "       [43, 16],\n",
            "       [27, 26],\n",
            "       [29, 26],\n",
            "       [26, 12],\n",
            "       [26, 12],\n",
            "       [43, 16],\n",
            "       [26, 12],\n",
            "       [26, 12],\n",
            "       [26, 12],\n",
            "       [40, 39],\n",
            "       [20, 10],\n",
            "       [33,  2],\n",
            "       [47, 38],\n",
            "       [46, 35],\n",
            "       [27,  0],\n",
            "       [46, 27],\n",
            "       [19, 13],\n",
            "       [41,  1],\n",
            "       [21,  6],\n",
            "       [26, 16],\n",
            "       [47, 39],\n",
            "       [43,  1],\n",
            "       [21, 12],\n",
            "       [45, 25],\n",
            "       [35,  4],\n",
            "       [26, 14],\n",
            "       [14,  0],\n",
            "       [46, 14],\n",
            "       [ 7,  5],\n",
            "       [47,  7],\n",
            "       [ 5,  1],\n",
            "       [43,  0],\n",
            "       [27,  0],\n",
            "       [27,  0],\n",
            "       [43,  1],\n",
            "       [43,  1],\n",
            "       [41, 16],\n",
            "       [ 4,  2],\n",
            "       [ 5,  4],\n",
            "       [ 5,  1],\n",
            "       [ 5,  4],\n",
            "       [ 5,  1],\n",
            "       [41,  1],\n",
            "       [41,  1],\n",
            "       [ 5,  1],\n",
            "       [41,  1],\n",
            "       [ 5,  1],\n",
            "       [ 5,  4],\n",
            "       [41,  1],\n",
            "       [41,  1],\n",
            "       [41,  1],\n",
            "       [41, 34],\n",
            "       [41,  1],\n",
            "       [41,  1],\n",
            "       [41, 40],\n",
            "       [41,  1],\n",
            "       [36,  3],\n",
            "       [45,  3],\n",
            "       [43,  3],\n",
            "       [10,  3],\n",
            "       [43,  3],\n",
            "       [22,  3],\n",
            "       [43, 22],\n",
            "       [43,  3],\n",
            "       [20, 10],\n",
            "       [18,  3],\n",
            "       [36,  3],\n",
            "       [45, 43],\n",
            "       [43,  3],\n",
            "       [45, 10],\n",
            "       [45,  3],\n",
            "       [45, 36],\n",
            "       [45,  3],\n",
            "       [45, 27],\n",
            "       [45, 43],\n",
            "       [12,  3],\n",
            "       [34,  2],\n",
            "       [12,  6],\n",
            "       [34,  2],\n",
            "       [10,  2],\n",
            "       [34,  2],\n",
            "       [30,  2],\n",
            "       [31, 13],\n",
            "       [16, 13],\n",
            "       [31, 13],\n",
            "       [16, 13],\n",
            "       [16, 13],\n",
            "       [26, 16],\n",
            "       [41,  1],\n",
            "       [15, 13],\n",
            "       [16, 13],\n",
            "       [16, 13],\n",
            "       [36,  3],\n",
            "       [16, 13],\n",
            "       [13, 12],\n",
            "       [26, 16],\n",
            "       [26, 16],\n",
            "       [16, 13],\n",
            "       [27, 14],\n",
            "       [29, 16],\n",
            "       [29, 27],\n",
            "       [29, 27],\n",
            "       [16, 14],\n",
            "       [13,  9],\n",
            "       [13,  9],\n",
            "       [44, 28],\n",
            "       [26, 12],\n",
            "       [47, 39],\n",
            "       [47, 39],\n",
            "       [38,  2],\n",
            "       [38,  6],\n",
            "       [38, 21],\n",
            "       [47, 12],\n",
            "       [38, 21],\n",
            "       [26, 16],\n",
            "       [43, 25],\n",
            "       [16, 12],\n",
            "       [43, 12],\n",
            "       [26, 12],\n",
            "       [43, 38],\n",
            "       [43,  6],\n",
            "       [12,  6],\n",
            "       [24, 23],\n",
            "       [46, 35],\n",
            "       [35,  3],\n",
            "       [46, 35],\n",
            "       [46, 35],\n",
            "       [46, 25],\n",
            "       [46, 35],\n",
            "       [46, 35],\n",
            "       [46, 19],\n",
            "       [47, 40],\n",
            "       [16, 10],\n",
            "       [14, 10],\n",
            "       [34, 10],\n",
            "       [42, 20],\n",
            "       [46, 35],\n",
            "       [42, 13],\n",
            "       [20,  0],\n",
            "       [10,  5],\n",
            "       [43, 17],\n",
            "       [43, 17],\n",
            "       [43,  5],\n",
            "       [17, 10],\n",
            "       [10,  0],\n",
            "       [43, 19],\n",
            "       [29, 27],\n",
            "       [43, 17],\n",
            "       [37, 19],\n",
            "       [45, 43],\n",
            "       [36, 13],\n",
            "       [10,  5],\n",
            "       [ 5,  1],\n",
            "       [10,  0],\n",
            "       [17, 10],\n",
            "       [27,  0],\n",
            "       [14,  0],\n",
            "       [46,  4],\n",
            "       [17, 10],\n",
            "       [43,  0],\n",
            "       [20, 10],\n",
            "       [43, 27],\n",
            "       [38, 22],\n",
            "       [42, 20],\n",
            "       [45,  3],\n",
            "       [45, 22],\n",
            "       [22,  3],\n",
            "       [43,  3],\n",
            "       [45, 22],\n",
            "       [45, 43],\n",
            "       [35, 18],\n",
            "       [46,  0],\n",
            "       [10,  0],\n",
            "       [46, 18],\n",
            "       [46, 18],\n",
            "       [43,  1],\n",
            "       [43,  1],\n",
            "       [43,  4],\n",
            "       [43,  1],\n",
            "       [43, 27],\n",
            "       [45, 43],\n",
            "       [27,  0],\n",
            "       [34, 27],\n",
            "       [34, 27],\n",
            "       [34, 27],\n",
            "       [ 4,  2],\n",
            "       [27,  0],\n",
            "       [30,  0],\n",
            "       [29, 27],\n",
            "       [ 5,  4],\n",
            "       [44, 21],\n",
            "       [ 6,  2],\n",
            "       [ 6,  2],\n",
            "       [ 6,  2],\n",
            "       [ 7,  6],\n",
            "       [27,  6],\n",
            "       [29, 27],\n",
            "       [35,  2],\n",
            "       [ 7,  5],\n",
            "       [ 7,  4],\n",
            "       [ 7,  5],\n",
            "       [ 7,  4],\n",
            "       [ 7,  4],\n",
            "       [24, 23],\n",
            "       [27,  6],\n",
            "       [27,  6],\n",
            "       [46,  2],\n",
            "       [ 5,  4],\n",
            "       [33, 32],\n",
            "       [32,  2],\n",
            "       [42, 30],\n",
            "       [42, 37],\n",
            "       [17, 10],\n",
            "       [18,  3],\n",
            "       [18,  3],\n",
            "       [10,  3],\n",
            "       [22,  3],\n",
            "       [22,  3],\n",
            "       [30,  3],\n",
            "       [43, 30],\n",
            "       [36,  3],\n",
            "       [36, 18],\n",
            "       [36, 18],\n",
            "       [36, 18],\n",
            "       [36, 18],\n",
            "       [36, 10],\n",
            "       [36,  0],\n",
            "       [36, 10],\n",
            "       [36,  0],\n",
            "       [36, 17],\n",
            "       [36, 17],\n",
            "       [36, 34],\n",
            "       [17, 10],\n",
            "       [36, 18],\n",
            "       [36, 10],\n",
            "       [36, 18],\n",
            "       [16, 13],\n",
            "       [16, 13],\n",
            "       [16, 13],\n",
            "       [13,  9],\n",
            "       [19, 13],\n",
            "       [26, 13],\n",
            "       [15, 14],\n",
            "       [14,  6],\n",
            "       [40, 16],\n",
            "       [27, 15],\n",
            "       [27, 15],\n",
            "       [15, 14],\n",
            "       [27, 15],\n",
            "       [43, 26],\n",
            "       [43, 27],\n",
            "       [43, 27],\n",
            "       [28, 19],\n",
            "       [43, 15],\n",
            "       [27, 15],\n",
            "       [27, 15],\n",
            "       [30, 27],\n",
            "       [27, 14],\n",
            "       [38,  6],\n",
            "       [36, 19],\n",
            "       [38,  6],\n",
            "       [39, 19],\n",
            "       [38, 12],\n",
            "       [38, 14],\n",
            "       [43, 21],\n",
            "       [38,  2],\n",
            "       [46, 33],\n",
            "       [34, 27],\n",
            "       [28, 21],\n",
            "       [ 5,  4],\n",
            "       [25,  0],\n",
            "       [41,  1],\n",
            "       [26,  7],\n",
            "       [26, 15],\n",
            "       [26, 12],\n",
            "       [26, 12],\n",
            "       [41, 16],\n",
            "       [12,  4],\n",
            "       [12,  7],\n",
            "       [24, 23],\n",
            "       [25, 23],\n",
            "       [25, 23],\n",
            "       [43, 25],\n",
            "       [25, 23],\n",
            "       [25, 19],\n",
            "       [43, 19],\n",
            "       [24, 23],\n",
            "       [36, 17],\n",
            "       [40, 19],\n",
            "       [41,  1],\n",
            "       [25,  9],\n",
            "       [16, 10],\n",
            "       [40,  1],\n",
            "       [40, 19],\n",
            "       [19, 16],\n",
            "       [40, 18],\n",
            "       [40,  1],\n",
            "       [29, 19],\n",
            "       [46, 35],\n",
            "       [46, 20],\n",
            "       [35,  2],\n",
            "       [19,  7],\n",
            "       [35,  0],\n",
            "       [14, 13]]), indices=array([[1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [1, 0],\n",
            "       [0, 1],\n",
            "       [0, 1]], dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVAO6-ddDh1z",
        "outputId": "59055070-fd5e-4ab1-ab11-82194bb19436"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 3)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 3)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred,3)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 3) (417,)\n",
            "precision 0.31654676258992803\n",
            "update_recall:  0.9496402877697842\n",
            "recall 0.9496402877697842\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 396.0, 21.0, 396.0, 855.0]\n",
            "TMP_RANK:  TopKV2(values=array([[ 5,  4,  1],\n",
            "       [41,  5,  1],\n",
            "       [ 5,  4,  1],\n",
            "       ...,\n",
            "       [35, 33,  7],\n",
            "       [42, 35, 32],\n",
            "       [42, 35,  8]]), indices=array([[0, 2, 1],\n",
            "       [2, 0, 1],\n",
            "       [0, 1, 2],\n",
            "       ...,\n",
            "       [0, 2, 1],\n",
            "       [1, 0, 2],\n",
            "       [1, 0, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLX7Wx9CDh4r",
        "outputId": "86bf2b52-4bba-4aab-ac13-c78b4b2ea6bd"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 4)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 4)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 4)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 4) (417,)\n",
            "precision 0.24400479616306955\n",
            "update_recall:  0.9760191846522782\n",
            "recall 0.9760191846522782\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 407.0, 10.0, 407.0, 1261.0]\n",
            "TMP_RANK:  TopKV2(values=array([[ 5,  4,  2,  1],\n",
            "       [41,  5,  2,  1],\n",
            "       [ 5,  4,  2,  1],\n",
            "       ...,\n",
            "       [35, 33, 32,  7],\n",
            "       [42, 35, 32,  8],\n",
            "       [42, 35, 32,  8]]), indices=array([[0, 2, 3, 1],\n",
            "       [2, 0, 3, 1],\n",
            "       [0, 1, 3, 2],\n",
            "       ...,\n",
            "       [0, 2, 3, 1],\n",
            "       [1, 0, 2, 3],\n",
            "       [1, 0, 3, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV492QBBDh7l",
        "outputId": "ce2e9db8-4aba-4e0b-a74a-0bc941884d73"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 5)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(417, 5) (417,)\n",
            "precision 0.1971223021582734\n",
            "update_recall:  0.9856115107913669\n",
            "recall 0.9856115107913669\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 411.0, 6.0, 411.0, 1674.0]\n",
            "TMP_RANK:  TopKV2(values=array([[34,  5,  4,  2,  1],\n",
            "       [41,  5,  4,  2,  1],\n",
            "       [29,  5,  4,  2,  1],\n",
            "       ...,\n",
            "       [42, 35, 33, 32,  7],\n",
            "       [42, 35, 33, 32,  8],\n",
            "       [42, 35, 33, 32,  8]]), indices=array([[4, 0, 2, 3, 1],\n",
            "       [2, 0, 4, 3, 1],\n",
            "       [4, 0, 1, 3, 2],\n",
            "       ...,\n",
            "       [4, 0, 2, 3, 1],\n",
            "       [1, 0, 4, 2, 3],\n",
            "       [1, 0, 4, 3, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGbi4DEZkwhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3368f1-fd70-4f5a-bdb5-2d5f4bc5690b"
      },
      "source": [
        "y_true = np.array(labels)\n",
        "final_predictions = np.array(final_predictions).squeeze()\n",
        "final_predictions.shape\n",
        "len(final_predictions[final_predictions==y_true])/len(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3918918918918919"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN1UA_4uBu_S"
      },
      "source": [
        "categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdD0JiYgEX4k"
      },
      "source": [
        "!cp -r /content/model_euclidean_SENT_BERT_cos_QC_attention_V3.zip \"/content/drive/My Drive/research_lo_content_taxonomy_classification\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}